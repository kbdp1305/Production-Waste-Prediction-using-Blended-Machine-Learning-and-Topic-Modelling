{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3m5yImkFfj6c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VU3NHMKfpd6U",
        "outputId": "aa4b87a1-3d11-4430-e9ab-fc692069fee9"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qMJA2xv6phNM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import gspread\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERMMNLaDwp5J"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_rows', None)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Tv-MpBKtRn1",
        "outputId": "2b6172a8-7b35-4cce-93b2-c1741cfe3d39"
      },
      "outputs": [],
      "source": [
        "df=pd.read_csv(\"/content/drive/MyDrive/LOMBA/fortex/data/Copy of 2023_us - 2023_us.csv.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oju-iMeI0TDq",
        "outputId": "621a5826-1daf-4c98-ee17-f56eac68e5ed"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8eYVAmHthVS",
        "outputId": "a6be776d-af2c-48f6-a6ef-0aa1be273d09"
      },
      "outputs": [],
      "source": [
        "# Initialize lists to store the results\n",
        "missing_percentage = []\n",
        "medians = []\n",
        "means = []\n",
        "unique_values = []\n",
        "\n",
        "# Loop through each column\n",
        "for col in df.columns:\n",
        "    # Calculate percentage of missing values\n",
        "    missing_percentage.append(df[col].isna().mean() * 100)\n",
        "\n",
        "    # Check if the column is numeric before calculating median and mean\n",
        "    if pd.api.types.is_numeric_dtype(df[col]):\n",
        "        medians.append(df[col].median())  # Calculate median\n",
        "        means.append(df[col].mean())  # Calculate mean\n",
        "    else:\n",
        "        medians.append(None)  # Set as None for non-numeric columns\n",
        "        means.append(None)  # Set as None for non-numeric columns\n",
        "\n",
        "    # Calculate number of unique values for every column\n",
        "    unique_values.append(df[col].nunique())\n",
        "\n",
        "# Create a summary DataFrame with the results\n",
        "summary_df = pd.DataFrame({\n",
        "    'Column': df.columns,\n",
        "    'Missing Percentage (%)': missing_percentage,\n",
        "    'Median': medians,\n",
        "    'Mean': means,\n",
        "    'Unique Values': unique_values\n",
        "})\n",
        "\n",
        "# Display the summary DataFrame\n",
        "print(summary_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBODyXyTnbDZ"
      },
      "outputs": [],
      "source": [
        "summary_df_sorted = summary_df.sort_values(by='Missing Percentage (%)', ascending=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tlnumWkVMr6r",
        "outputId": "7752fcfe-66b0-4ae6-8658-5fe4764ccd95"
      },
      "outputs": [],
      "source": [
        "summary_df_sorted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inJVnFFAuWqD"
      },
      "outputs": [],
      "source": [
        "filtered_columns = summary_df[summary_df['Missing Percentage (%)'] < 70]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "rpyHI47Rwej6",
        "outputId": "f8411fa9-92c7-4ead-903e-9f1046e6cb3e"
      },
      "outputs": [],
      "source": [
        "filtered_columns.sort_values(by='Missing Percentage (%)', ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfTnOXKKw5on",
        "outputId": "e897f8b2-440f-4f03-fe88-07a68b71c1c4"
      },
      "outputs": [],
      "source": [
        "filtered_columns = summary_df[summary_df['Missing Percentage (%)'] < 70]\n",
        "\n",
        "# Initialize lists for classification\n",
        "numeric_columns = []\n",
        "categorical_columns = []\n",
        "other_columns = []\n",
        "\n",
        "# Classify columns into numeric, categorical, and other categories\n",
        "for col in filtered_columns['Column']:\n",
        "    if pd.api.types.is_numeric_dtype(df[col]):\n",
        "        numeric_columns.append(col)\n",
        "    elif pd.api.types.is_categorical_dtype(df[col]) or df[col].dtype == 'object':\n",
        "        categorical_columns.append(col)\n",
        "    else:\n",
        "        other_columns.append(col)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fuz-4teoWCf",
        "outputId": "70f0b59b-910c-457f-f56a-185becff98af"
      },
      "outputs": [],
      "source": [
        "len(categorical_columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSu0I7PuoY9B",
        "outputId": "ea391b35-7e9b-43d3-bb94-250e83652514"
      },
      "outputs": [],
      "source": [
        "len(numeric_columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b70zKmmKoesd",
        "outputId": "82c79a2f-92e2-4bba-cee8-3a139e3fc646"
      },
      "outputs": [],
      "source": [
        "len(other_columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXabtrD7ol9R",
        "outputId": "81f6cbb6-d8f1-438f-dbc7-53584cd828a5"
      },
      "outputs": [],
      "source": [
        "79+26"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVbyBrDM4n6k"
      },
      "outputs": [],
      "source": [
        "# List of columns to exclude\n",
        "columns_to_exclude = [\n",
        "    \"1. YEAR\", \"2. TRIFD\", \"3. FRS ID\", \"4. FACILITY NAME\", \"5. STREET ADDRESS\",\n",
        "    \"6. CITY\", \"7. COUNTY\", \"8. ST\", \"9. ZIP\", \"10. BIA\", \"11. TRIBE\", \"12. LATITUDE\",\n",
        "    \"13. LONGITUDE\", \"14. HORIZONTAL DATUM\", \"15. PARENT CO NAME\", \"16. PARENT CO DB NUM\",\n",
        "    \"17. STANDARD PARENT CO NAME\", \"18. FOREIGN PARENT CO NAME\", \"19. FOREIGN PARENT CO DB NUM\",\n",
        "    \"20. STANDARD FOREIGN PARENT CO NAME\", \"23. INDUSTRY SECTOR\", \"24. PRIMARY SIC\",\n",
        "    \"25. SIC 2\", \"26. SIC 3\", \"27. SIC 4\", \"28. SIC 5\", \"29. SIC 6\", \"30. PRIMARY NAICS\",\n",
        "    \"31. NAICS 2\", \"32. NAICS 3\", \"33. NAICS 4\", \"34. NAICS 5\", \"35. NAICS 6\",\n",
        "    \"36. DOC_CTRL_NUM\", \"39. TRI CHEMICAL/COMPOUND ID\", \"40. CAS#\", \"41. SRS ID\"\n",
        "]\n",
        "\n",
        "# Exclude those columns from the DataFrame\n",
        "filtered_columns = filtered_columns[~filtered_columns['Column'].isin(columns_to_exclude)]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "cQgBTNlA4qPG",
        "outputId": "c69a7dca-0453-4684-c824-635b77be64d0"
      },
      "outputs": [],
      "source": [
        "filtered_columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "D_AvhyJq7vMA",
        "outputId": "48000c04-dec1-4fe5-e1b3-34a799b0407e"
      },
      "outputs": [],
      "source": [
        "filtered_columns.sort_values(by='Missing Percentage (%)', ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w68vKWlHon8j",
        "outputId": "dc03ddc1-95e8-4235-82f1-acdcbb885ec5"
      },
      "outputs": [],
      "source": [
        "filtered_columns[filtered_columns['Unique Values']==1]['Column'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZz2JC5Qo3le"
      },
      "outputs": [],
      "source": [
        "# Get the columns to remove\n",
        "columns_to_remove = filtered_columns[filtered_columns['Unique Values'] == 1]['Column'].values\n",
        "\n",
        "# Filter the `filtered_columns` DataFrame to exclude rows where the 'Column' is in `columns_to_remove`\n",
        "filtered_columns = filtered_columns[~filtered_columns['Column'].isin(columns_to_remove)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "g_DBpVyKmJBJ",
        "outputId": "87a3c77a-f7e3-470e-e381-787137963f10"
      },
      "outputs": [],
      "source": [
        "\n",
        "filtered_columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5Qa5UnmnWw4"
      },
      "source": [
        "# Filter by big relation with target\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yp0cWVfam0Tq"
      },
      "outputs": [],
      "source": [
        "df_=df.copy()\n",
        "df_=df_[filtered_columns['Column'].values]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 721
        },
        "id": "MpWHiqtNrn5G",
        "outputId": "f77ea90b-5b80-44bb-ee10-9373f93f3d0e"
      },
      "outputs": [],
      "source": [
        "df_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REqFtN8Drwpa",
        "outputId": "b15ab32f-fc4d-48b6-80db-0b6095e4b1a7"
      },
      "outputs": [],
      "source": [
        "# Initialize lists to store the results\n",
        "missing_percentage = []\n",
        "medians = []\n",
        "means = []\n",
        "unique_values = []\n",
        "\n",
        "# Loop through each column\n",
        "for col in df_.columns:\n",
        "    # Calculate percentage of missing values\n",
        "    missing_percentage.append(df_[col].isna().mean() * 100)\n",
        "\n",
        "    # Check if the column is numeric before calculating median and mean\n",
        "    if pd.api.types.is_numeric_dtype(df_[col]):\n",
        "        medians.append(df_[col].median())  # Calculate median\n",
        "        means.append(df_[col].mean())  # Calculate mean\n",
        "    else:\n",
        "        medians.append(None)  # Set as None for non-numeric columns\n",
        "        means.append(None)  # Set as None for non-numeric columns\n",
        "\n",
        "    # Calculate number of unique values for every column\n",
        "    unique_values.append(df_[col].nunique())\n",
        "\n",
        "# Create a summary DataFrame with the results\n",
        "summary_df_ = pd.DataFrame({\n",
        "    'Column': df_.columns,\n",
        "    'Missing Percentage (%)': missing_percentage,\n",
        "    'Median': medians,\n",
        "    'Mean': means,\n",
        "    'Unique Values': unique_values\n",
        "})\n",
        "\n",
        "# Display the summary DataFrame\n",
        "print(summary_df_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQG-AUrgsB6a"
      },
      "outputs": [],
      "source": [
        "summary_df_ = summary_df_.sort_values(by=\"Missing Percentage (%)\", ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "Fnmm2wzRsOwQ",
        "outputId": "db1211dd-fddd-4664-f344-420561cea395"
      },
      "outputs": [],
      "source": [
        "summary_df_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLnrzEuKsfax"
      },
      "outputs": [],
      "source": [
        "target='119. PRODUCTION WSTE (8.1-8.7)'\n",
        "df_numeric = df_.select_dtypes(include=['number'])\n",
        "\n",
        "# Calculate the correlation with the target column\n",
        "correlation_with_target = df_numeric.corr()[target].sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fionWT-qs6EH"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9r2XNbT-JCl"
      },
      "outputs": [],
      "source": [
        "correlation_with_target = df_numeric.corr()[target].sort_values(ascending=False)\n",
        "\n",
        "# Filter out columns with less than 1% correlation (both positive and negative)\n",
        "columns_to_keep = correlation_with_target[abs(correlation_with_target) >= 0.01].index\n",
        "\n",
        "# Filter the original DataFrame to keep only the selected columns\n",
        "df_filtered = df_numeric[columns_to_keep]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ov7WA3jso1g"
      },
      "outputs": [],
      "source": [
        "df_['121. PROD_RATIO_OR_ ACTIVITY']=df_['121. PROD_RATIO_OR_ ACTIVITY'].fillna(df_['121. PROD_RATIO_OR_ ACTIVITY'].mode()[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "wrv--rR4_UOP",
        "outputId": "a7edaf37-db14-4a1d-907b-db0a4b0bff22"
      },
      "outputs": [],
      "source": [
        "df_['122. 8.9 - PRODUCTION RATIO']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "zXa5XLAb-T5S",
        "outputId": "fac66438-cc16-49b3-9ed0-55ef331284b4"
      },
      "outputs": [],
      "source": [
        "df_=df_filtered.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdGjQv2Ot_c8"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_rows', None)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "tPgOLTajtaPq",
        "outputId": "6f885f39-6594-4502-8d28-8e2e48b7e52f"
      },
      "outputs": [],
      "source": [
        "from sklearn.impute import KNNImputer\n",
        "\n",
        "# Separate the numeric columns from the non-numeric columns\n",
        "numeric_cols = df_.select_dtypes(include=['number']).columns\n",
        "non_numeric_cols = df_.select_dtypes(exclude=['number']).columns\n",
        "\n",
        "# Initialize the KNNImputer with the number of neighbors\n",
        "knn_imputer = KNNImputer(n_neighbors=5)\n",
        "\n",
        "# Apply KNN imputation to the numeric columns\n",
        "df_numeric_imputed = pd.DataFrame(knn_imputer.fit_transform(df_[numeric_cols]), columns=numeric_cols)\n",
        "\n",
        "# Combine the imputed numeric columns with the non-numeric columns\n",
        "df_imputed = pd.concat([df_numeric_imputed, df_[non_numeric_cols]], axis=1)\n",
        "\n",
        "# Optionally, display the imputed DataFrame\n",
        "df_imputed.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBq3UFXC_rdp",
        "outputId": "14e7cdc8-e0ac-481a-8d5f-55a9851ea5d1"
      },
      "outputs": [],
      "source": [
        "# Initialize lists to store the results\n",
        "missing_percentage = []\n",
        "medians = []\n",
        "means = []\n",
        "unique_values = []\n",
        "df_=df_imputed.copy()\n",
        "# Loop through each column\n",
        "for col in df_.columns:\n",
        "    # Calculate percentage of missing values\n",
        "    missing_percentage.append(df_[col].isna().mean() * 100)\n",
        "\n",
        "    # Check if the column is numeric before calculating median and mean\n",
        "    if pd.api.types.is_numeric_dtype(df_[col]):\n",
        "        medians.append(df_[col].median())  # Calculate median\n",
        "        means.append(df_[col].mean())  # Calculate mean\n",
        "    else:\n",
        "        medians.append(None)  # Set as None for non-numeric columns\n",
        "        means.append(None)  # Set as None for non-numeric columns\n",
        "\n",
        "    # Calculate number of unique values for every column\n",
        "    unique_values.append(df_[col].nunique())\n",
        "\n",
        "# Create a summary DataFrame with the results\n",
        "summary_df_ = pd.DataFrame({\n",
        "    'Column': df_.columns,\n",
        "    'Missing Percentage (%)': missing_percentage,\n",
        "    'Median': medians,\n",
        "    'Mean': means,\n",
        "    'Unique Values': unique_values\n",
        "})\n",
        "\n",
        "# Display the summary DataFrame\n",
        "print(summary_df_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uG_RAl9Y_zid"
      },
      "outputs": [],
      "source": [
        "summary_df_ = summary_df_.sort_values(by=\"Missing Percentage (%)\", ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "tRhdnrGN_6N1",
        "outputId": "fa96d1d7-ac4f-4f32-f84e-4b8ea17b91a2"
      },
      "outputs": [],
      "source": [
        "summary_df_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EH5gB887ABL3"
      },
      "outputs": [],
      "source": [
        "df_.to_csv(\"/content/drive/MyDrive/LOMBA/fortex/data/fortex_final_data.csv\",index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7Y2zOhwutLs"
      },
      "outputs": [],
      "source": [
        "categorical_cols = df_.select_dtypes(exclude=['number']).columns\n",
        "df_imputed = df_[categorical_cols].apply(lambda col: col.fillna(col.mode()[0]), axis=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "eDA-yHWlu_wD",
        "outputId": "16ce0a24-b2f5-4786-e5b2-7633ecce5112"
      },
      "outputs": [],
      "source": [
        "df_imputed.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "id": "6u1KqCXOvCMa",
        "outputId": "83f193d7-7b14-4d82-e039-5d0a82081af2"
      },
      "outputs": [],
      "source": [
        "df_imputed.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHy4LTjrvMEq"
      },
      "outputs": [],
      "source": [
        "df_final=df_.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YerjpL6J1Cn1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3DyctW_MvVRC"
      },
      "outputs": [],
      "source": [
        "df_final[df_imputed.columns]=df_imputed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8TZ2OkZvZLi",
        "outputId": "b74eee57-ab33-4714-93e4-ab3466513081"
      },
      "outputs": [],
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Check for columns with missing values\n",
        "missing_columns = df_final.columns[df_final.isnull().any()]\n",
        "print(\"Columns with missing values:\", missing_columns)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3oo5VG7t1k66"
      },
      "outputs": [],
      "source": [
        "df_final=df_final.drop(['41. SRS ID'],axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZ5TQrct2HDV"
      },
      "outputs": [],
      "source": [
        "df_final=df_final.drop(['2. TRIFD'],axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ivb8M4af14n0"
      },
      "outputs": [],
      "source": [
        "df_final['122. 8.9 - PRODUCTION RATIO']=df['122. 8.9 - PRODUCTION RATIO'].fillna(df['122. 8.9 - PRODUCTION RATIO'].median())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8FYVGjiIvbps"
      },
      "outputs": [],
      "source": [
        "df_final.to_csv(\"/content/drive/MyDrive/LOMBA/fortex/data/final_cleaned_data2.csv\",index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 836
        },
        "id": "1eAqxVeu1D_S",
        "outputId": "23cce2b1-9f55-4944-b724-bb5a54948fe9"
      },
      "outputs": [],
      "source": [
        "df_final.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3K196wP_85iy",
        "outputId": "9c6fc179-41e2-4f06-d7e4-3c06419b7aa8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df=pd.read_csv(\"/content/drive/MyDrive/LOMBA/fortex/data/final_cleaned_data.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 516
        },
        "id": "4CzHsuPrX5qM",
        "outputId": "0484f99b-1178-4225-dd89-efae3dc1fae0"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "P9Lb-sdXXzZx",
        "outputId": "05446081-ed05-46f8-8a4d-a6887357a34d"
      },
      "outputs": [],
      "source": [
        "# List of features you want to recover\n",
        "features_to_recover =  [\n",
        "    '51. 5.1 - FUGITIVE AIR', '52. 5.2 - STACK AIR', '53. 5.3 - WATER', '54. 5.4 - UNDERGROUND',\n",
        "    '55. 5.4.1 - UNDERGROUND CL I', '56. 5.4.2 - UNDERGROUND C II-V', '57. 5.5.1 - LANDFILLS',\n",
        "    '58. 5.5.1A - RCRA C LANDFILL', '59. 5.5.1B - OTHER LANDFILLS', '60. 5.5.2 - LAND TREATMENT',\n",
        "    '61. 5.5.3 - SURFACE IMPNDMNT', '62. 5.5.3A - RCRA SURFACE IM', '63. 5.5.3B - OTHER SURFACE I',\n",
        "    '64. 5.5.4 - OTHER DISPOSAL', '65. ON-SITE RELEASE TOTAL', '66. 6.1 - POTW - TRNS RLSE',\n",
        "    '67. 6.1 - POTW - TRNS TRT', '68. POTW - TOTAL TRANSFERS', '69. 6.2 - M10', '70. 6.2 - M41',\n",
        "    '71. 6.2 - M62', '72. 6.2 - M40 METAL', '73. 6.2 - M61 METAL', '74. 6.2 - M71', '75. 6.2 - M81',\n",
        "    '76. 6.2 - M82', '77. 6.2 - M72', '78. 6.2 - M63', '79. 6.2 - M66', '80. 6.2 - M67', '81. 6.2 - M64',\n",
        "    '82. 6.2 - M65', '83. 6.2 - M73', '84. 6.2 - M79', '85. 6.2 - M90', '86. 6.2 - M94', '87. 6.2 - M99',\n",
        "    '88. OFF-SITE RELEASE TOTAL', '89. 6.2 - M20', '90. 6.2 - M24', '91. 6.2 - M26', '92. 6.2 - M28',\n",
        "    '93. 6.2 - M93', '94. OFF-SITE RECYCLED TOTAL', '95. 6.2 - M56', '96. 6.2 - M92', '97. OFF-SITE ENERGY RECOVERY T',\n",
        "    '98. 6.2 - M40 NON-METAL', '99. 6.2 - M50', '100. 6.2 - M54', '101. 6.2 - M61 NON-METAL', '102. 6.2 - M69',\n",
        "    '103. 6.2 - M95', '104. OFF-SITE TREATED TOTAL', '105. 6.2 - UNCLASSIFIED'\n",
        "]\n",
        "\n",
        "# Check if all the columns are in 'df'\n",
        "columns_found = [col for col in features_to_recover if col in df.columns]\n",
        "columns_not_found = [col for col in features_to_recover if col not in df.columns]\n",
        "\n",
        "# If there are missing columns, print them\n",
        "if columns_not_found:\n",
        "    print(f\"Columns not found in df: {columns_not_found}\")\n",
        "\n",
        "# Recover the columns from 'df' and add them to 'df_final'\n",
        "df_final[columns_found] = df[columns_found]\n",
        "\n",
        "# Verify if the columns have been added\n",
        "print(f\"Columns added to df_final: {columns_found}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        },
        "id": "0ZRsSUWlYPia",
        "outputId": "7f1a8723-c116-4531-8de1-0ee40f9cf504"
      },
      "outputs": [],
      "source": [
        "df_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6V0okOCRQ1F"
      },
      "outputs": [],
      "source": [
        "df_final=df_no_outliers.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPMWODlTAc2Z"
      },
      "outputs": [],
      "source": [
        "df_final=df_.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZtEu3INYAmDG",
        "outputId": "fdf40b2d-9e25-496c-93d2-8a6c733fb93b"
      },
      "outputs": [],
      "source": [
        "pip install category-encoders\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqJyqePsAfGW",
        "outputId": "70ee2ac0-c1f5-4ff4-f645-1d10be4f81f3"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Convert all categorical columns to string type\n",
        "categorical_cols = df_final.select_dtypes(include=['object']).columns\n",
        "df_final[categorical_cols] = df_final[categorical_cols].astype(str)\n",
        "\n",
        "# Step 2: Define the target and features\n",
        "target_column = '119. PRODUCTION WSTE (8.1-8.7)'  # Assuming this is the target column\n",
        "X = df_final.drop(columns=[target_column])  # Features (excluding target)\n",
        "y = df_final[target_column]  # Target (Production Waste)\n",
        "\n",
        "# Step 3: Apply Label Encoding to all categorical columns\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "for col in X.select_dtypes(include=['object']).columns:\n",
        "    X[col] = label_encoder.fit_transform(X[col])\n",
        "\n",
        "# Step 4: Apply Standard Scaling to the features\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Only scale the numeric columns (exclude the target and categorical columns)\n",
        "numeric_cols = X.select_dtypes(include=['float64', 'int64']).columns\n",
        "X[numeric_cols] = scaler.fit_transform(X[numeric_cols])\n",
        "\n",
        "# Step 5: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 6: Initialize and train the RandomForestRegressor model\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "# Step 7: Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 8: Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 9: Evaluate the model\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f'Mean Absolute Error (MAE): {mae}')\n",
        "print(f'Mean Squared Error (MSE): {mse}')\n",
        "print(f'R2 Score: {r2}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_b7AO3Hlvgt9",
        "outputId": "561b413a-55d0-42a6-efa5-f21c1761b508"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Convert all categorical columns to string type\n",
        "# Convert all categorical columns to string to ensure consistency for encoding\n",
        "categorical_cols = df_final.select_dtypes(include=['object']).columns\n",
        "df_final[categorical_cols] = df_final[categorical_cols].astype(str)\n",
        "\n",
        "print(\"Categorical columns after conversion:\", categorical_cols)\n",
        "\n",
        "# Step 2: Define the target and features\n",
        "target_column = '119. PRODUCTION WSTE (8.1-8.7)'  # Assuming this is the target column\n",
        "X = df_final.drop(columns=[target_column])  # Features (excluding target)\n",
        "y = df_final[target_column]  # Target (Production Waste)\n",
        "\n",
        "# Step 3: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# # Step 4: Define a function to encode categorical columns\n",
        "# def encode_categorical_columns(X_train, X_test, categorical_cols):\n",
        "#     label_encoders = {}\n",
        "\n",
        "#     # Apply LabelEncoder to each categorical column\n",
        "#     for col in categorical_cols:\n",
        "#         le = LabelEncoder()\n",
        "#         le.fit(X_train[col])  # Fit encoder only on training data\n",
        "#         X_train[col] = le.transform(X_train[col])\n",
        "\n",
        "#         # Apply the same encoder to the test set, ignoring unknown labels\n",
        "#         X_test[col] = X_test[col].apply(lambda x: le.transform([x])[0] if x in le.classes_ else -1)  # Map unseen labels to -1\n",
        "\n",
        "#         label_encoders[col] = le  # Store the encoder for future use\n",
        "\n",
        "#     return X_train, X_test, label_encoders\n",
        "\n",
        "# # Step 5: Encode categorical columns in both training and test sets\n",
        "# X_train, X_test, label_encoders = encode_categorical_columns(X_train, X_test, categorical_cols)\n",
        "\n",
        "# Step 6: Initialize and train the RandomForestRegressor model\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "# Step 7: Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 8: Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 9: Evaluate the model\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f'Mean Absolute Error (MAE): {mae}')\n",
        "print(f'Mean Squared Error (MSE): {mse}')\n",
        "print(f'R2 Score: {r2}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "id": "C1gUnIvhYw7e",
        "outputId": "8ee90bbb-ea9d-4717-9754-9fea3a48be78"
      },
      "outputs": [],
      "source": [
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TU63AMppRsJc",
        "outputId": "637f2bea-8bc4-4f1f-ccda-8f09dac59ba8"
      },
      "outputs": [],
      "source": [
        "df_final['119. PRODUCTION WSTE (8.1-8.7)'].mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wGRik3-OSj2",
        "outputId": "7102641e-ea27-4809-82cf-29f96aeb28d3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Assuming y_test and y_pred are already defined (true values and predictions)\n",
        "mse = mean_squared_error(y_test, y_pred)  # Calculate Mean Squared Error\n",
        "rmse = np.sqrt(mse)  # Calculate Root Mean Squared Error\n",
        "\n",
        "rmse\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pm3xOTS0NoZp",
        "outputId": "b8e8db11-28a8-475a-c1f4-2fc28fef5f25"
      },
      "outputs": [],
      "source": [
        "rmse = np.sqrt(90203262568691.48)\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "id": "ex6gG3_3PbwJ",
        "outputId": "4fcd3dea-64dc-4d9d-a116-889c919e90ae"
      },
      "outputs": [],
      "source": [
        "df_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rczYeJOx_I6V",
        "outputId": "7855ebc4-f233-41e7-a89b-7fc47aeb0188"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def detect_outliers(df):\n",
        "    \"\"\"\n",
        "    Function to detect outliers in all numeric columns of a dataframe using the IQR method.\n",
        "    The function returns the indices of outliers.\n",
        "    \"\"\"\n",
        "    outlier_indices = set()  # Using a set to avoid duplicate indices\n",
        "\n",
        "    # Loop through each numeric column in the dataframe\n",
        "    for col in df.select_dtypes(include=['number']).columns:\n",
        "        # Check if the column has at least two unique values (avoid constant columns)\n",
        "        if df[col].nunique() > 1:\n",
        "            # Drop rows with NaN values in the column (or use fillna if preferred)\n",
        "            df_col = df[col].dropna()\n",
        "\n",
        "            # Check if the column is not empty after dropping NaNs\n",
        "            if len(df_col) == 0:\n",
        "                continue  # Skip empty columns\n",
        "\n",
        "            Q1 = df_col.quantile(0.25)  # First quartile (25%)\n",
        "            Q3 = df_col.quantile(0.75)  # Third quartile (75%)\n",
        "            IQR = Q3 - Q1  # Interquartile range\n",
        "\n",
        "            # Handle division by zero if IQR is zero\n",
        "            if IQR == 0:\n",
        "                continue  # No outliers if IQR is zero\n",
        "\n",
        "            # Define the lower and upper bounds for outliers\n",
        "            lower_bound = Q1 - 1.5 * IQR\n",
        "            upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "            # Detect outliers (values outside the bounds)\n",
        "            outlier = df[col][(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
        "\n",
        "            # Add the indices of the outliers to the set\n",
        "            outlier_indices.update(outlier.index)\n",
        "\n",
        "            # Print the number of outliers in the column\n",
        "            print(f\"Number of outliers in {col}: {outlier.count()}\")\n",
        "\n",
        "    return outlier_indices\n",
        "\n",
        "# Example usage:\n",
        "# Assuming 'df_final' is your DataFrame with data\n",
        "outliers_indices = detect_outliers(df_final)\n",
        "\n",
        "# Now you can filter out the outliers from the DataFrame\n",
        "df_no_outliers = df_final.drop(index=outliers_indices)\n",
        "\n",
        "print(f\"Filtered DataFrame without outliers: \\n{df_no_outliers}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLfqtxMsPYlj"
      },
      "outputs": [],
      "source": [
        "df_no_outliers.to_csv(\"/content/drive/MyDrive/LOMBA/fortex/data/no_outliers.csv\",index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yh4kU-BX_P0H",
        "outputId": "b183107a-73d9-4ddb-b3a8-f1ffb81c1bba"
      },
      "outputs": [],
      "source": [
        "df_final['119. PRODUCTION WSTE (8.1-8.7)'].mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmM09SC-2ZhK",
        "outputId": "ce0b03c1-5ef6-40f8-bcb3-caa62b886ac4"
      },
      "outputs": [],
      "source": [
        "# List of categorical columns to check unique values\n",
        "categorical_columns = ['4. FACILITY NAME', '5. STREET ADDRESS', '6. CITY', '7. COUNTY',\n",
        "                       '8. ST', '14. HORIZONTAL DATUM', '15. PARENT CO NAME', '16. PARENT CO DB NUM',\n",
        "                       '17. STANDARD PARENT CO NAME', '21. FEDERAL FACILITY', '23. INDUSTRY SECTOR',\n",
        "                       '37. CHEMICAL', '38. ELEMENTAL METAL INCLUDED', '39. TRI CHEMICAL/COMPOUND ID',\n",
        "                       '40. CAS#', '42. CLEAN AIR ACT CHEMICAL', '43. CLASSIFICATION', '44. METAL',\n",
        "                       '45. METAL CATEGORY', '46. CARCINOGEN', '47. PBT', '48. PFAS', '49. FORM TYPE',\n",
        "                       '50. UNIT OF MEASURE', '121. PROD_RATIO_OR_ ACTIVITY']\n",
        "\n",
        "# Print unique values for each categorical column\n",
        "for col in categorical_columns:\n",
        "    print(f\"Unique values in '{col}':\\n\", df_final[col].unique(), \"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8_c-FF3y5xb",
        "outputId": "103e3eb7-e3b9-4bbe-9886-e7b20ab70c2b"
      },
      "outputs": [],
      "source": [
        "# List of categorical columns to check unique values\n",
        "categorical_columns = ['4. FACILITY NAME', '5. STREET ADDRESS', '6. CITY', '7. COUNTY',\n",
        "                       '8. ST', '14. HORIZONTAL DATUM', '15. PARENT CO NAME', '16. PARENT CO DB NUM',\n",
        "                       '17. STANDARD PARENT CO NAME', '21. FEDERAL FACILITY', '23. INDUSTRY SECTOR',\n",
        "                       '37. CHEMICAL', '38. ELEMENTAL METAL INCLUDED', '39. TRI CHEMICAL/COMPOUND ID',\n",
        "                       '40. CAS#', '42. CLEAN AIR ACT CHEMICAL', '43. CLASSIFICATION', '44. METAL',\n",
        "                       '45. METAL CATEGORY', '46. CARCINOGEN', '47. PBT', '48. PFAS', '49. FORM TYPE',\n",
        "                       '50. UNIT OF MEASURE', '121. PROD_RATIO_OR_ ACTIVITY']\n",
        "\n",
        "# Print unique values for each categorical column\n",
        "for col in categorical_columns:\n",
        "    print(f\"Unique values in '{col}':\\n\", df[col].unique(), \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYjUoCEwDMCz"
      },
      "source": [
        "# FE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Atwb0sZJzQwv"
      },
      "outputs": [],
      "source": [
        "df=pd.read_csv(\"/content/drive/MyDrive/LOMBA/fortex/data/fortex_final_data.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9QZA23pFePG",
        "outputId": "1824b606-1da6-4a89-e2ba-d73e684b46d0"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ejR-CB0tFjd9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# 1. Feature Grouping: Define key groups related to waste and emissions.\n",
        "waste_columns = ['59. 5.5.1B - OTHER LANDFILLS',\n",
        "                 '60. 5.5.2 - LAND TREATMENT',\n",
        "                 '62. 5.5.3A - RCRA SURFACE IM',\n",
        "                 '63. 5.5.3B - OTHER SURFACE I',\n",
        "                 '64. 5.5.4 - OTHER DISPOSAL',\n",
        "                 '58. 5.5.1A - RCRA C LANDFILL']\n",
        "\n",
        "emission_columns = ['51. 5.1 - FUGITIVE AIR', '52. 5.2 - STACK AIR', '53. 5.3 - WATER']\n",
        "\n",
        "# 2. Creating composite features: Sum emissions, waste, and other groups\n",
        "df['Total Waste'] = df[waste_columns].sum(axis=1, skipna=True)\n",
        "df['Total Emissions'] = df[emission_columns].sum(axis=1, skipna=True)\n",
        "\n",
        "# 3. Handling Chemical and Risk Features: Create a \"Risk Factor\"\n",
        "df['Risk Factor'] = np.where(df['46. CARCINOGEN'] == 'Yes', 1, 0) + \\\n",
        "                    np.where(df['47. PBT'] == 'Yes', 1, 0) + \\\n",
        "                    np.where(df['48. PFAS'] == 'Yes', 1, 0)\n",
        "\n",
        "# 4. Waste Disposal Method: Combine on-site vs off-site releases\n",
        "df[' +'] = df['109. 8.1A - ON-SITE CONTAINED'] + df['110. 8.1B - ON-SITE OTHER']\n",
        "df['Off-Site Waste Disposal'] = df['111. 8.1C - OFF-SITE CONTAIN'] + df['112. 8.1D - OFF-SITE OTHER R']\n",
        "\n",
        "# 5. Create a ratio of on-site vs off-site waste disposal\n",
        "# df['On-Site to Off-Site Ratio'] = df['On-Site Waste Disposal'] / (df['Off-Site Waste Disposal'] + 1)  # Adding 1 to avoid division by 0\n",
        "\n",
        "# 6. Interaction between Total Emissions and Waste: Create a feature to indicate the relation between emissions and waste\n",
        "df['Emissions to Waste Ratio'] = df['Total Emissions'] / (df['Total Waste'] + 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "fhTrCo32GNDP"
      },
      "outputs": [],
      "source": [
        "# df['Waste Disposal by Industry'] = df['22. INDUSTRY SECTOR CODE'].astype(str) + \" \" + df['On-Site Waste Disposal'].astype(str)\n",
        "\n",
        "# Ratio between Emissions and Waste per Chemical Category (Carcinogen, PBT, PFAS)\n",
        "df['Emissions to Waste per Carcinogen'] = df['Total Emissions'] / (df['Total Waste'] + 1) * np.where(df['46. CARCINOGEN'] == 'Yes', 1, 0)\n",
        "df['Emissions to Waste per PBT'] = df['Total Emissions'] / (df['Total Waste'] + 1) * np.where(df['47. PBT'] == 'Yes', 1, 0)\n",
        "df['Emissions to Waste per PFAS'] = df['Total Emissions'] / (df['Total Waste'] + 1) * np.where(df['48. PFAS'] == 'Yes', 1, 0)\n",
        "\n",
        "# Metal Categories Interaction\n",
        "df['Metal Waste Disposal'] = np.where(df['44. METAL'] == 'Yes', df['Total Waste'], 0)\n",
        "df['Non-Metal Waste Disposal'] = np.where(df['44. METAL'] != 'Yes', df['Total Waste'], 0)\n",
        "\n",
        "# Additional Waste Disposal Types (e.g., surface impoundments, landfills)\n",
        "df['Landfill to Surface Ratio'] = df['59. 5.5.1B - OTHER LANDFILLS'] / (df['62. 5.5.3A - RCRA SURFACE IM'] + 1)\n",
        "\n",
        "# # Ratios and Aggregations\n",
        "# df['Total Waste to Facility Ratio'] = df['Total Waste'] / (df['22. INDUSTRY SECTOR CODE'].astype(str).map(df['22. INDUSTRY SECTOR CODE'].value_counts()) + 1)\n",
        "# df['Total Emissions to Facility Ratio'] = df['Total Emissions'] / (df['22. INDUSTRY SECTOR CODE'].astype(str).map(df['22. INDUSTRY SECTOR CODE'].value_counts()) + 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "HMFoqKAyGm7B"
      },
      "outputs": [],
      "source": [
        "# 1. Aggregated Release Features (e.g., POTW and Other Releases)\n",
        "df['Total POTW Releases'] = df['66. 6.1 - POTW - TRNS RLSE'] + df['67. 6.1 - POTW - TRNS TRT'] + df['68. POTW - TOTAL TRANSFERS']\n",
        "df['Total Off-Site Releases'] = df['88. OFF-SITE RELEASE TOTAL'] + df['94. OFF-SITE RECYCLED TOTAL'] + df['97. OFF-SITE ENERGY RECOVERY T'] + df['104. OFF-SITE TREATED TOTAL']\n",
        "df['Total Releases'] = df['Total POTW Releases'] + df['Total Off-Site Releases'] + df['107. TOTAL RELEASES']\n",
        "\n",
        "# 2. Energy Recovery and Recycling Ratios\n",
        "df['Energy Recovery to Recycling'] = (df['113. 8.2 - ENERGY RECOVER ON'] + df['114. 8.3 - ENERGY RECOVER OF']) / (df['115. 8.4 - RECYCLING ON SITE'] + df['116. 8.5 - RECYCLING OFF SIT'] + 1)\n",
        "\n",
        "# 3. Treatment Features (on-site vs off-site)\n",
        "df['On-Site Treatment Ratio'] = (df['117. 8.6 - TREATMENT ON SITE']) / (df['118. 8.7 - TREATMENT OFF SITE'] + 1)\n",
        "\n",
        "\n",
        "# 5. Categorical Feature Encoding\n",
        "df['Chemical'] = np.where(df['37. CHEMICAL'] == 'Yes', 1, 0)\n",
        "df['Elemental Metal'] = np.where(df['38. ELEMENTAL METAL INCLUDED'] == 'Yes', 1, 0)\n",
        "df['Clean Air Act Chemical'] = np.where(df['42. CLEAN AIR ACT CHEMICAL'] == 'Yes', 1, 0)\n",
        "\n",
        "# 6. Ratio Features: Energy Recovery to Total Releases, Total Transfers to Waste\n",
        "df['Energy Recovery to Total Releases'] = (df['113. 8.2 - ENERGY RECOVER ON'] + df['114. 8.3 - ENERGY RECOVER OF']) / (df['Total Releases'] + 1)\n",
        "df['Total Transfers to Waste'] = (df['68. POTW - TOTAL TRANSFERS'] + df['106. 6.2 - TOTAL TRANSFER']) / (df['119. PRODUCTION WSTE (8.1-8.7)'] + 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "FcyG2lB-VX7B"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "uTHUTMKeMDlp"
      },
      "outputs": [],
      "source": [
        "df['Total M-Series'] = df[['69. 6.2 - M10', '70. 6.2 - M41', '71. 6.2 - M62', '75. 6.2 - M81',\n",
        "                            '76. 6.2 - M82', '79. 6.2 - M66', '80. 6.2 - M67', '81. 6.2 - M64',\n",
        "                            '82. 6.2 - M65', '83. 6.2 - M73', '84. 6.2 - M79', '85. 6.2 - M90',\n",
        "                            '86. 6.2 - M94', '87. 6.2 - M99', '89. 6.2 - M20', '90. 6.2 - M24',\n",
        "                            '91. 6.2 - M26', '92. 6.2 - M28', '93. 6.2 - M93', '95. 6.2 - M56',\n",
        "                            '96. 6.2 - M92', '98. 6.2 - M40 NON-METAL', '99. 6.2 - M50',\n",
        "                            '100. 6.2 - M54', '101. 6.2 - M61 NON-METAL', '102. 6.2 - M69',\n",
        "                            '103. 6.2 - M95']].sum(axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "mH0__IN5MMip"
      },
      "outputs": [],
      "source": [
        "df['Average M-Series'] = df[['69. 6.2 - M10', '70. 6.2 - M41', '71. 6.2 - M62', '75. 6.2 - M81',\n",
        "                            '76. 6.2 - M82', '79. 6.2 - M66', '80. 6.2 - M67', '81. 6.2 - M64',\n",
        "                            '82. 6.2 - M65', '83. 6.2 - M73', '84. 6.2 - M79', '85. 6.2 - M90',\n",
        "                            '86. 6.2 - M94', '87. 6.2 - M99', '89. 6.2 - M20', '90. 6.2 - M24',\n",
        "                            '91. 6.2 - M26', '92. 6.2 - M28', '93. 6.2 - M93', '95. 6.2 - M56',\n",
        "                            '96. 6.2 - M92', '98. 6.2 - M40 NON-METAL', '99. 6.2 - M50',\n",
        "                            '100. 6.2 - M54', '101. 6.2 - M61 NON-METAL', '102. 6.2 - M69',\n",
        "                            '103. 6.2 - M95']].mean(axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "K9IPvDleMPgk"
      },
      "outputs": [],
      "source": [
        "# # 2. Ratios Between M-series values (to identify patterns or relationships)\n",
        "# df['M10_to_M41'] = df['69. 6.2 - M10'] / (df['70. 6.2 - M41'] + 1)\n",
        "\n",
        "# # 3. Categorization of Non-Metal Features\n",
        "# df['M40_NON_METAL'] = np.where(df['98. 6.2 - M40 NON-METAL'] > 0, 1, 0)\n",
        "# df['M61_NON_METAL'] = np.where(df['101. 6.2 - M61 NON-METAL'] > 0, 1, 0)\n",
        "\n",
        "# 4. Off-Site Release and Treatment Aggregation\n",
        "df['Total Off-Site Releases'] = df[['88. OFF-SITE RELEASE TOTAL', '94. OFF-SITE RECYCLED TOTAL',\n",
        "                                    '97. OFF-SITE ENERGY RECOVERY T', '104. OFF-SITE TREATED TOTAL']].sum(axis=1)\n",
        "df['Total Off-Site Treatment'] = df[['109. 8.1A - ON-SITE CONTAINED', '110. 8.1B - ON-SITE OTHER',\n",
        "                                     '111. 8.1C - OFF-SITE CONTAIN', '112. 8.1D - OFF-SITE OTHER R']].sum(axis=1)\n",
        "\n",
        "# 5. Production Ratio Feature (Interaction)\n",
        "df['Production_to_Off_Site_Treatment'] = df['119. PRODUCTION WSTE (8.1-8.7)'] / (df['Total Off-Site Treatment'] + 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "cnv667oyMT2v"
      },
      "outputs": [],
      "source": [
        "\n",
        "m_series_columns = ['69. 6.2 - M10', '70. 6.2 - M41', '71. 6.2 - M62', '75. 6.2 - M81',\n",
        "                    '76. 6.2 - M82', '79. 6.2 - M66', '80. 6.2 - M67', '81. 6.2 - M64',\n",
        "                    '82. 6.2 - M65', '83. 6.2 - M73', '84. 6.2 - M79', '85. 6.2 - M90',\n",
        "                    '86. 6.2 - M94', '87. 6.2 - M99', '89. 6.2 - M20', '90. 6.2 - M24',\n",
        "                    '91. 6.2 - M26', '92. 6.2 - M28', '93. 6.2 - M93', '95. 6.2 - M56',\n",
        "                    '96. 6.2 - M92', '98. 6.2 - M40 NON-METAL', '99. 6.2 - M50',\n",
        "                    '100. 6.2 - M54', '101. 6.2 - M61 NON-METAL', '102. 6.2 - M69',\n",
        "                    '103. 6.2 - M95']\n",
        "def flag_zeros(df, columns):\n",
        "    for col in columns:\n",
        "        df[f'{col}_zero_flag'] = np.where(df[col] == 0, 1, 0)\n",
        "    return df\n",
        "\n",
        "df = flag_zeros(df, m_series_columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "qlts1AcqMdi9"
      },
      "outputs": [],
      "source": [
        "def flag_high_values(df, columns, threshold=1000):\n",
        "    for col in columns:\n",
        "        df[f'{col}_high_flag'] = np.where(df[col] > threshold, 1, 0)\n",
        "    return df\n",
        "\n",
        "df = flag_high_values(df, m_series_columns, threshold=1000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Z9iTv98WMfql"
      },
      "outputs": [],
      "source": [
        "# 5. Flagging Non-Metal Related Features\n",
        "df['M40_NON_METAL_flag'] = np.where(df['98. 6.2 - M40 NON-METAL'] > 0, 1, 0)\n",
        "df['M61_NON_METAL_flag'] = np.where(df['101. 6.2 - M61 NON-METAL'] > 0, 1, 0)\n",
        "\n",
        "# 6. Flagging Off-Site Release (when total exceeds a threshold, e.g., 1000)\n",
        "df['Off_Site_Release_High_flag'] = np.where(df['Total Off-Site Releases'] > 1000, 1, 0)\n",
        "\n",
        "# 7. Flagging Production Waste (production waste greater than a threshold, e.g., 5000)\n",
        "df['Production_Waste_High_flag'] = np.where(df['119. PRODUCTION WSTE (8.1-8.7)'] > 5000, 1, 0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "H4xe4XCUMlsJ"
      },
      "outputs": [],
      "source": [
        "# Mean, Standard Deviation, Median of Waste and Emissions\n",
        "df['Mean Waste'] = df[waste_columns].mean(axis=1, skipna=True)\n",
        "df['Mean Emissions'] = df[emission_columns].mean(axis=1, skipna=True)\n",
        "\n",
        "df['Std Waste'] = df[waste_columns].std(axis=1, skipna=True)\n",
        "df['Std Emissions'] = df[emission_columns].std(axis=1, skipna=True)\n",
        "\n",
        "df['Median Waste'] = df[waste_columns].median(axis=1, skipna=True)\n",
        "df['Median Emissions'] = df[emission_columns].median(axis=1, skipna=True)\n",
        "\n",
        "# Binned Features for Waste and Emissions\n",
        "df['Waste Bin'] = pd.cut(df['Total Waste'], bins=[-np.inf, 1000, 5000, np.inf], labels=['Low', 'Medium', 'High'])\n",
        "df['Emissions Bin'] = pd.cut(df['Total Emissions'], bins=[-np.inf, 500, 2000, np.inf], labels=['Low', 'Medium', 'High'])\n",
        "\n",
        "# Cumulative Sum Features for Waste and Emissions\n",
        "df['Cumulative Waste'] = df['Total Waste'].cumsum()\n",
        "df['Cumulative Emissions'] = df['Total Emissions'].cumsum()\n",
        "\n",
        "# Interaction Feature: Risk Factor and Total Waste\n",
        "df['Risk Factor x Total Waste'] = df['Risk Factor'] * df['Total Waste']\n",
        "\n",
        "# Waste to Emissions Ratio\n",
        "df['Waste to Emissions Ratio'] = df['Total Waste'] / (df['Total Emissions'] + 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "9oUt0W0LM1no"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# List of chemicals\n",
        "chemicals = np.array([\n",
        "    'Hydrochloric acid (acid aerosols including mists, vapors, gas, fog, and other airborne forms of any particle size)',\n",
        "    'Vanadium compounds', 'p-Cresol', 'Zinc compounds', 'Nonylphenol', 'Manganese compounds',\n",
        "    'Lead', 'Xylene (mixed isomers)', 'Polycyclic aromatic compounds', 'Nitric acid', 'Xylene (mixed isomers)',\n",
        "    'Pentachloroethane', 'Phenol', 'Lead', 'Nitrate compounds (water dissociable; reportable only when in aqueous solution)',\n",
        "    'Manganese compounds', 'Methanol', 'Lead compounds', 'Dimethylamine', 'Dioxin and dioxin-like compounds',\n",
        "    'Nitric acid', 'Diisocyanates', 'Toluene', 'Nitrate compounds (water dissociable; reportable only when in aqueous solution)',\n",
        "    'Nonylphenol Ethoxylates', 'Lead compounds', 'Mercury  And Mercury Compounds', 'Methanol', 'Diisocyanates',\n",
        "    'Lead', 'Nickel', 'Maleic anhydride', 'Hydrogen sulfide', 'Ammonia', 'Chlorophenols', 'Zinc compounds',\n",
        "    'Copper compounds', 'Manganese  And Manganese Compounds', 'Manganese', 'Mercury  And Mercury Compounds',\n",
        "    'Copper  And Copper Compounds', 'Ethylene glycol', 'Copper', 'n-Hexane', 'Toluene',\n",
        "    'Lead  And Lead Compounds', 'Phthalic anhydride', 'Nickel', 'Nitric acid', 'Diisocyanates',\n",
        "    'Methyl isobutyl ketone', 'Lead  And Lead Compounds', 'Lead', 'Cobalt', 'n-Hexane', 'Copper',\n",
        "    'Nitrate compounds (water dissociable; reportable only when in aqueous solution)', 'Certain glycol ethers',\n",
        "    'Mercury compounds', 'Nickel', 'Copper', 'Chromium  and Chromium Compounds(except for chromite ore mined in the Transvaal Region)',\n",
        "    'Copper', 'Nitrate compounds (water dissociable; reportable only when in aqueous solution)',\n",
        "    'Formaldehyde', 'Copper', 'Manganese', 'Manganese  And Manganese Compounds', 'Hexachlorocyclopentadiene',\n",
        "    'Certain glycol ethers', 'Biphenyl', '1,2-Butylene oxide', 'Ethyl acrylate', 'Methanol', 'Nickel',\n",
        "    'Sulfuric acid (acid aerosols including mists, vapors, gas, fog, and other airborne forms of any particle size)',\n",
        "    'Methanol', 'Pentobarbital sodium', 'Dichloromethane', 'Lead', 'n-Hexane', 'Hydrogen cyanide', 'Lead',\n",
        "    'Hydrogen fluoride', 'Xylene (mixed isomers)', 'Dioxin and dioxin-like compounds', 'Chromium',\n",
        "    'Certain glycol ethers', 'Ammonia', 'Dicyclopentadiene', 'Benzo[g,h,i]perylene', 'Methanol', 'Ammonia',\n",
        "    'Ammonia', 'Manganese  And Manganese Compounds', 'Ethylbenzene', 'Methanol', 'Nitrate compounds (water dissociable; reportable only when in aqueous solution)',\n",
        "    'n-Hexane', 'Manganese', 'Dioxin and dioxin-like compounds', 'Styrene', 'Lead', 'Aldrin', 'Chromium',\n",
        "    'Nickel', 'Polycyclic aromatic compounds', 'Manganese', 'Zinc compounds', 'Arsenic compounds', 'Naphthalene',\n",
        "    'Diethanolamine', 'Molybdenum trioxide', 'Zinc compounds', 'Lead  And Lead Compounds', 'Acetaldehyde',\n",
        "    'Manganese compounds', 'Toluene', 'Dioxin and dioxin-like compounds', 'Toluene', 'Certain glycol ethers',\n",
        "    'Diisocyanates', 'Zinc (fume or dust)', 'Arsenic', 'n-Hexane', '1,2,4-Trimethylbenzene', 'Polycyclic aromatic compounds',\n",
        "    'Formaldehyde', 'Benzene', 'Nonylphenol Ethoxylates', 'Styrene', 'Manganese', 'Chromium', 'Methanol',\n",
        "    'Polycyclic aromatic compounds', 'Chromium', 'Mercury  And Mercury Compounds', 'Arsenic compounds', 'Manganese',\n",
        "    'Nitrate compounds (water dissociable; reportable only when in aqueous solution)',\n",
        "    'Chromium  and Chromium Compounds(except for chromite ore mined in the Transvaal Region)',\n",
        "    'Polycyclic aromatic compounds', 'Copper', 'Barium compounds (except for barium sulfate (CAS No. 7727-43-7))',\n",
        "    'Chromium', 'Toluene', 'Ammonia', 'n-Butyl alcohol', 'Nitrate compounds (water dissociable; reportable only when in aqueous solution)',\n",
        "    'Toluene', 'Methanol'\n",
        "])\n",
        "\n",
        "# Define categories for grouping\n",
        "chemical_categories = {\n",
        "    'Heavy Metals': ['Lead', 'Mercury', 'Copper', 'Zinc', 'Nickel', 'Arsenic', 'Manganese', 'Chromium', 'Vanadium', 'Cobalt'],\n",
        "    'Acids': ['Hydrochloric acid', 'Nitric acid', 'Sulfuric acid', 'Acetaldehyde', 'Methanol'],\n",
        "    'Aromatic Compounds': ['Xylene', 'Styrene', 'Ethylbenzene', 'Toluene', 'Naphthalene', 'Polycyclic aromatic compounds'],\n",
        "    'Other Organic Compounds': ['Methanol', 'Dimethylamine', 'Formaldehyde', 'Diisocyanates'],\n",
        "    'Industrial Chemicals': ['Nonylphenol', 'Nitrate compounds', 'Biphenyl', 'Ethyl acrylate', 'Dichloromethane', 'Maleic anhydride'],\n",
        "    'Glycol Ethers': ['Certain glycol ethers'],\n",
        "    'Pesticides': ['Aldrin']\n",
        "}\n",
        "\n",
        "# Create a function to categorize chemicals\n",
        "def categorize_chemical(chemical_name):\n",
        "    for category, chemicals in chemical_categories.items():\n",
        "        for chem in chemicals:\n",
        "            if chem in chemical_name:\n",
        "                return category\n",
        "    return 'Other'  # If not found in any category\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "UswFPjlvgTKW"
      },
      "outputs": [],
      "source": [
        "# Define categories for grouping\n",
        "chemical_categories = {\n",
        "    'Heavy Metals': ['Lead', 'Mercury', 'Copper', 'Zinc', 'Nickel', 'Arsenic', 'Manganese', 'Chromium', 'Vanadium', 'Cobalt'],\n",
        "    'Acids': ['Hydrochloric acid', 'Nitric acid', 'Sulfuric acid', 'Acetaldehyde', 'Methanol'],\n",
        "    'Aromatic Compounds': ['Xylene', 'Styrene', 'Ethylbenzene', 'Toluene', 'Naphthalene', 'Polycyclic aromatic compounds'],\n",
        "    'Other Organic Compounds': ['Methanol', 'Dimethylamine', 'Formaldehyde', 'Diisocyanates'],\n",
        "    'Industrial Chemicals': ['Nonylphenol', 'Nitrate compounds', 'Biphenyl', 'Ethyl acrylate', 'Dichloromethane', 'Maleic anhydride'],\n",
        "    'Glycol Ethers': ['Certain glycol ethers'],\n",
        "    'Pesticides': ['Aldrin']\n",
        "}\n",
        "\n",
        "# Create a function to categorize chemicals\n",
        "def categorize_chemical(chemical_name):\n",
        "    for category, chemicals in chemical_categories.items():\n",
        "        for chem in chemicals:\n",
        "            if chem in chemical_name:\n",
        "                return category\n",
        "    return 'Other'  # If not found in any category\n",
        "\n",
        "# Apply categorization and create a new 'Category' column\n",
        "df['Category'] = df['37. CHEMICAL'].apply(categorize_chemical)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-BHQCRqgje2",
        "outputId": "8a54476c-fdb5-498d-fe14-7af3edd90c4f"
      },
      "outputs": [],
      "source": [
        "def classify_risk_and_hazard(chemical_name):\n",
        "    # Define high, medium, and low risk chemicals based on hazardous nature\n",
        "    high_risk = ['Lead', 'Mercury', 'Arsenic', 'Chromium', 'Dioxin', 'Aldrin']\n",
        "    medium_risk = ['Xylene', 'Toluene', 'Manganese', 'Nickel', 'Zinc']\n",
        "    low_risk = ['Methanol', 'Formaldehyde', 'Ethylbenzene', 'Copper']\n",
        "\n",
        "    hazard_types = {\n",
        "        'Toxic': ['Lead', 'Mercury', 'Arsenic'],\n",
        "        'Corrosive': ['Hydrochloric acid', 'Sulfuric acid', 'Nitric acid'],\n",
        "        'Flammable': ['Xylene', 'Toluene', 'Ethylbenzene'],\n",
        "        'Environmental': ['Pesticides', 'Nonylphenol']\n",
        "    }\n",
        "\n",
        "    # Classify risk\n",
        "    risk_level = 'Low'\n",
        "    for chem in high_risk:\n",
        "        if chem in chemical_name:\n",
        "            risk_level = 'High'\n",
        "            break\n",
        "    for chem in medium_risk:\n",
        "        if chem in chemical_name:\n",
        "            risk_level = 'Medium'\n",
        "            break\n",
        "\n",
        "    # Classify hazard\n",
        "    hazard = 'None'\n",
        "    for hazard_type, chemicals in hazard_types.items():\n",
        "        for chem in chemicals:\n",
        "            if chem in chemical_name:\n",
        "                hazard = hazard_type\n",
        "                break\n",
        "\n",
        "    return risk_level, hazard\n",
        "\n",
        "# Apply the function to classify risk and hazard\n",
        "df[['Risk_Level', 'Hazard_Type']] = df['37. CHEMICAL'].apply(lambda x: pd.Series(classify_risk_and_hazard(x)))\n",
        "\n",
        "# Feature Engineering for production waste prediction (example features)\n",
        "df['CHEMICAL_Category'] = df['37. CHEMICAL'].apply(categorize_chemical)\n",
        "df['Risk_Score'] = df['Risk_Level'].apply(lambda x: 3 if x == 'High' else (2 if x == 'Medium' else 1))\n",
        "df['Hazard_Score'] = df['Hazard_Type'].apply(lambda x: 3 if x == 'Toxic' else (2 if x == 'Corrosive' else 1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8x0qA76M2cX",
        "outputId": "dbccfa08-96f6-4612-e955-26fe6b98d5b0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def detect_outliers(df, columns):\n",
        "    \"\"\"\n",
        "    Function to detect outliers in specified columns of a dataframe using the IQR method.\n",
        "    The function returns the indices of outliers.\n",
        "    \"\"\"\n",
        "    outlier_indices = set()  # Using a set to avoid duplicate indices\n",
        "\n",
        "    # Loop through the specified columns in the dataframe\n",
        "    for col in columns:\n",
        "        if col in df.columns:\n",
        "            # Check if the column is numeric (either int or float)\n",
        "            if pd.api.types.is_numeric_dtype(df[col]):\n",
        "                # Check if the column has at least two unique values (avoid constant columns)\n",
        "                if df[col].nunique() > 1:\n",
        "                    # Drop rows with NaN values in the column (or use fillna if preferred)\n",
        "                    df_col = df[col].dropna()\n",
        "\n",
        "                    # Check if the column is not empty after dropping NaNs\n",
        "                    if len(df_col) == 0:\n",
        "                        continue  # Skip empty columns\n",
        "\n",
        "                    Q1 = df_col.quantile(0.25)  # First quartile (25%)\n",
        "                    Q3 = df_col.quantile(0.75)  # Third quartile (75%)\n",
        "                    IQR = Q3 - Q1  # Interquartile range\n",
        "\n",
        "                    # Handle division by zero if IQR is zero\n",
        "                    if IQR == 0:\n",
        "                        continue  # No outliers if IQR is zero\n",
        "\n",
        "                    # Define the lower and upper bounds for outliers\n",
        "                    lower_bound = Q1 - 1.5 * IQR\n",
        "                    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "                    # Detect outliers (values outside the bounds)\n",
        "                    outlier = df[col][(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
        "\n",
        "                    # Add the indices of the outliers to the set\n",
        "                    outlier_indices.update(outlier.index)\n",
        "\n",
        "                    # Print the number of outliers in the column\n",
        "                    print(f\"Number of outliers in {col}: {outlier.count()}\")\n",
        "            else:\n",
        "                print(f\"Skipping column {col} because it is not numeric.\")\n",
        "\n",
        "    return outlier_indices\n",
        "\n",
        "# List of columns you want to check for outliers\n",
        "columns_to_check = [\n",
        "    '22. INDUSTRY SECTOR CODE', '51. 5.1 - FUGITIVE AIR', '52. 5.2 - STACK AIR', '53. 5.3 - WATER',\n",
        "    '55. 5.4.1 - UNDERGROUND CL I', '56. 5.4.2 - UNDERGROUND C II-V', '58. 5.5.1A - RCRA C LANDFILL',\n",
        "    '59. 5.5.1B - OTHER LANDFILLS', '60. 5.5.2 - LAND TREATMENT', '62. 5.5.3A - RCRA SURFACE IM',\n",
        "    '63. 5.5.3B - OTHER SURFACE I', '64. 5.5.4 - OTHER DISPOSAL', '65. ON-SITE RELEASE TOTAL',\n",
        "    '66. 6.1 - POTW - TRNS RLSE', '67. 6.1 - POTW - TRNS TRT', '68. POTW - TOTAL TRANSFERS', '69. 6.2 - M10',\n",
        "    '70. 6.2 - M41', '71. 6.2 - M62', '75. 6.2 - M81', '76. 6.2 - M82', '79. 6.2 - M66', '80. 6.2 - M67',\n",
        "    '81. 6.2 - M64', '82. 6.2 - M65', '83. 6.2 - M73', '84. 6.2 - M79', '85. 6.2 - M90', '86. 6.2 - M94',\n",
        "    '87. 6.2 - M99', '88. OFF-SITE RELEASE TOTAL', '89. 6.2 - M20', '90. 6.2 - M24', '91. 6.2 - M26',\n",
        "    '92. 6.2 - M28', '93. 6.2 - M93', '94. OFF-SITE RECYCLED TOTAL', '95. 6.2 - M56', '96. 6.2 - M92',\n",
        "    '97. OFF-SITE ENERGY RECOVERY T', '98. 6.2 - M40 NON-METAL', '99. 6.2 - M50', '100. 6.2 - M54',\n",
        "    '101. 6.2 - M61 NON-METAL', '102. 6.2 - M69', '103. 6.2 - M95', '104. OFF-SITE TREATED TOTAL',\n",
        "    '106. 6.2 - TOTAL TRANSFER', '107. TOTAL RELEASES', '109. 8.1A - ON-SITE CONTAINED',\n",
        "    '110. 8.1B - ON-SITE OTHER', '111. 8.1C - OFF-SITE CONTAIN', '112. 8.1D - OFF-SITE OTHER R',\n",
        "    '113. 8.2 - ENERGY RECOVER ON', '114. 8.3 - ENERGY RECOVER OF', '115. 8.4 - RECYCLING ON SITE',\n",
        "    '116. 8.5 - RECYCLING OFF SIT', '117. 8.6 - TREATMENT ON SITE', '118. 8.7 - TREATMENT OFF SITE',\n",
        "    '119. PRODUCTION WSTE (8.1-8.7)', '122. 8.9 - PRODUCTION RATIO', '121. PROD_RATIO_OR_ ACTIVITY',\n",
        "    '21. FEDERAL FACILITY', '37. CHEMICAL', '38. ELEMENTAL METAL INCLUDED', '42. CLEAN AIR ACT CHEMICAL',\n",
        "    '43. CLASSIFICATION', '44. METAL', '45. METAL CATEGORY', '46. CARCINOGEN', '47. PBT', '48. PFAS',\n",
        "    '49. FORM TYPE', '50. UNIT OF MEASURE'\n",
        "]\n",
        "\n",
        "# Example usage:\n",
        "df_final = df.copy()  # Assuming 'df_final' is your DataFrame with data\n",
        "outliers_indices = detect_outliers(df_final, columns_to_check)\n",
        "\n",
        "# Now you can filter out the outliers from the DataFrame\n",
        "df_no_outliers = df_final.drop(index=outliers_indices)\n",
        "\n",
        "print(f\"Filtered DataFrame without outliers: \\n{df_no_outliers}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wSWewdEYMfQ",
        "outputId": "0b380b2b-2dcd-428d-874d-62a67902e5dd"
      },
      "outputs": [],
      "source": [
        "len(df_no_outliers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGaIdXE4YGhA",
        "outputId": "e981918a-ed7d-4afc-ee65-96d0ead6ebc5"
      },
      "outputs": [],
      "source": [
        "df['37. CHEMICAL'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldL9ze1XY6v9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "oow6pulANHla",
        "outputId": "f2c17047-9c47-400e-ca1e-963e90a51f08"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hbj0E1TCZ52O"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        },
        "id": "QhyxMtTLZzMD",
        "outputId": "ebe47e8f-f49e-42bd-85a2-433f5ca62542"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-iAh3TmZtc9",
        "outputId": "0e79b72a-fff8-4f16-cd0e-b2bcff602e69"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Assuming df_no_outliers is already preprocessed\n",
        "df_final = df_no_outliers.copy()\n",
        "\n",
        "# Step 1: Convert all categorical columns to string type\n",
        "categorical_cols = df_final.select_dtypes(include=['object','bool']).columns\n",
        "df_final[categorical_cols] = df_final[categorical_cols].astype(str)\n",
        "\n",
        "# Step 2: Define the target and features\n",
        "target_column = '119. PRODUCTION WSTE (8.1-8.7)'  # Assuming this is the target column\n",
        "X = df_final.drop(columns=[target_column])  # Features (excluding target)\n",
        "y = df_final[target_column]  # Target (Production Waste)\n",
        "\n",
        "# Step 3: Apply Label Encoding to all categorical columns\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "for col in X.select_dtypes(include=['object','category']).columns:\n",
        "    X[col] = label_encoder.fit_transform(X[col])\n",
        "\n",
        "# Step 4: Apply Standard Scaling to the features\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Only scale the numeric columns (exclude the target and categorical columns)\n",
        "numeric_cols = X.select_dtypes(include=['float64', 'int64']).columns\n",
        "X[numeric_cols] = scaler.fit_transform(X[numeric_cols])\n",
        "\n",
        "# Step 5: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 6: Initialize and train the RandomForestRegressor model\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "# Step 7: Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 8: Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 9: Evaluate the model\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f'Mean Absolute Error (MAE): {mae}')\n",
        "print(f'Mean Squared Error (MSE): {mse}')\n",
        "print(f'R2 Score: {r2}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5DTzJmxeczuW",
        "outputId": "5e6f8379-ba4f-4511-eca5-0f886738e79e"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "# Step 6: Initialize and train the GradientBoostingRegressor model\n",
        "model = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
        "# X_train=X_train.drop(['Total Waste to Facility Ratio','Total Emissions to Facility Ratio'],axis=1)\n",
        "# X_test=X_test.drop(['Total Waste to Facility Ratio','Total Emissions to Facility Ratio'],axis=1)\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f'Mean Absolute Error (MAE): {mae}')\n",
        "print(f'Mean Squared Error (MSE): {mse}')\n",
        "print(f'R2 Score: {r2}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQQaGq40c4RC",
        "outputId": "5af87a78-939f-4ed8-b067-a47c56141077"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "# Step 6: Initialize and train the XGBoost model\n",
        "model = xgb.XGBRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f'Mean Absolute Error (MAE): {mae}')\n",
        "print(f'Mean Squared Error (MSE): {mse}')\n",
        "print(f'R2 Score: {r2}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jMGL9Dpc8e0",
        "outputId": "ca4896b6-2e9c-4da8-b4c5-5c31996b7a64"
      },
      "outputs": [],
      "source": [
        "# Step 6: Initialize and train the CatBoostRegressor model\n",
        "from catboost import CatBoostRegressor\n",
        "\n",
        "model = CatBoostRegressor(iterations=1000, depth=10, learning_rate=0.1, loss_function='RMSE',  random_state=42, verbose=100)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 7: Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 8: Evaluate the model\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f'Mean Absolute Error (MAE): {mae}')\n",
        "print(f'Mean Squared Error (MSE): {mse}')\n",
        "print(f'R2 Score: {r2}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJH4Ft1ZdH-N",
        "outputId": "ba065d21-7b7b-4a77-a933-a760ed352535"
      },
      "outputs": [],
      "source": [
        "# Step 6: Initialize and train the LGBMRegressor model\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Initialize the LGBMRegressor model\n",
        "model = LGBMRegressor(n_estimators=1000,\n",
        "                      max_depth=10,\n",
        "                      learning_rate=0.1,\n",
        "                      objective='regression',\n",
        "                      random_state=42,\n",
        "                      verbose=100)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 7: Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 8: Evaluate the model\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f'Mean Absolute Error (MAE): {mae}')\n",
        "print(f'Mean Squared Error (MSE): {mse}')\n",
        "print(f'R2 Score: {r2}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R8TGF1BdbIRo"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "# Step 6: Initialize and train the GradientBoostingRegressor model\n",
        "model = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
        "# X_train=X_train.drop(['Total Waste to Facility Ratio','Total Emissions to Facility Ratio'],axis=1)\n",
        "# X_test=X_test.drop(['Total Waste to Facility Ratio','Total Emissions to Facility Ratio'],axis=1)\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f'Mean Absolute Error (MAE): {mae}')\n",
        "print(f'Mean Squared Error (MSE): {mse}')\n",
        "print(f'R2 Score: {r2}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXEsSr32kosH",
        "outputId": "fa6b88b0-f146-4086-90c7-26363e34a618"
      },
      "outputs": [],
      "source": [
        "len(df_no_outliers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKk_Xxc8hs1B"
      },
      "source": [
        "# WITH FE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9vjuDdKhu_j",
        "outputId": "0d96d583-807e-425a-c04f-b5845887f8b6"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Assuming df_no_outliers is already preprocessed\n",
        "df_final = df_no_outliers.copy()\n",
        "\n",
        "# Step 1: Convert all categorical columns to string type\n",
        "categorical_cols = df_final.select_dtypes(include=['object','bool']).columns\n",
        "df_final[categorical_cols] = df_final[categorical_cols].astype(str)\n",
        "\n",
        "# Step 2: Define the target and features\n",
        "target_column = '119. PRODUCTION WSTE (8.1-8.7)'  # Assuming this is the target column\n",
        "X = df_final.drop(columns=[target_column])  # Features (excluding target)\n",
        "y = df_final[target_column]  # Target (Production Waste)\n",
        "\n",
        "# Step 3: Apply Label Encoding to all categorical columns\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "for col in X.select_dtypes(include=['object','category']).columns:\n",
        "    X[col] = label_encoder.fit_transform(X[col])\n",
        "\n",
        "# Step 4: Apply Standard Scaling to the features\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Only scale the numeric columns (exclude the target and categorical columns)\n",
        "numeric_cols = X.select_dtypes(include=['float64', 'int64']).columns\n",
        "X[numeric_cols] = scaler.fit_transform(X[numeric_cols])\n",
        "\n",
        "# Step 5: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 6: Initialize and train the RandomForestRegressor model\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "# Step 7: Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 8: Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 9: Evaluate the model\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f'Mean Absolute Error (MAE): {mae}')\n",
        "print(f'Mean Squared Error (MSE): {mse}')\n",
        "print(f'R2 Score: {r2}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mxA9aEjFiJxp",
        "outputId": "8ccda2ce-c289-471b-91ad-56987ed9ce24"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "# Step 6: Initialize and train the GradientBoostingRegressor model\n",
        "model = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
        "# X_train=X_train.drop(['Total Waste to Facility Ratio','Total Emissions to Facility Ratio'],axis=1)\n",
        "# X_test=X_test.drop(['Total Waste to Facility Ratio','Total Emissions to Facility Ratio'],axis=1)\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f'Mean Absolute Error (MAE): {mae}')\n",
        "print(f'Mean Squared Error (MSE): {mse}')\n",
        "print(f'R2 Score: {r2}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFTS1iuFiNQB",
        "outputId": "9ed21bae-f7b0-4a14-a773-0db2491ecf87"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "# Step 6: Initialize and train the XGBoost model\n",
        "model = xgb.XGBRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f'Mean Absolute Error (MAE): {mae}')\n",
        "print(f'Mean Squared Error (MSE): {mse}')\n",
        "print(f'R2 Score: {r2}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QnnVf2161Rxo",
        "outputId": "7ed9c50c-4d24-4168-8104-960ff9f4d1be"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "import numpy as np\n",
        "from pyswarm import pso\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "import pandas as pd\n",
        "\n",
        "# Assume the following variables are already set up:\n",
        "# df_final, target_column, X, y, X_train, X_test, y_train, y_test\n",
        "\n",
        "# Define the objective function for PSO\n",
        "def objective_function(params):\n",
        "    learning_rate, max_depth, n_estimators, subsample, colsample_bytree = params\n",
        "\n",
        "    # Train XGBoost model with the given parameters\n",
        "    model = xgb.XGBRegressor(\n",
        "        learning_rate=learning_rate,\n",
        "        max_depth=int(max_depth),  # max_depth should be an integer\n",
        "        n_estimators=int(n_estimators),  # n_estimators should be an integer\n",
        "        subsample=subsample,\n",
        "        colsample_bytree=colsample_bytree,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on the test set\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate the mean absolute error (MAE)\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "    return mae  # We want to minimize MAE\n",
        "\n",
        "# Define the boundaries for the hyperparameters\n",
        "# Bounds: [learning_rate, max_depth, n_estimators, subsample, colsample_bytree]\n",
        "lb = [0.01, 3, 50, 0.5, 0.5]  # Lower bounds of the parameters\n",
        "ub = [0.1, 8, 100, 0.9, 0.9]  # Upper bounds of the parameters (narrowed range)\n",
        "\n",
        "# Perform PSO optimization with minimal swarm size and iterations\n",
        "best_params, _ = pso(objective_function, lb, ub, swarmsize=5, maxiter=10, debug=True)\n",
        "\n",
        "# Print the best parameters found by PSO\n",
        "print(f\"Best parameters found by PSO: {best_params}\")\n",
        "\n",
        "# Extract the best parameters\n",
        "learning_rate, max_depth, n_estimators, subsample, colsample_bytree = best_params\n",
        "\n",
        "# Train the final XGBoost model using the optimized parameters\n",
        "final_model = xgb.XGBRegressor(\n",
        "    learning_rate=learning_rate,\n",
        "    max_depth=int(max_depth),\n",
        "    n_estimators=int(n_estimators),\n",
        "    subsample=subsample,\n",
        "    colsample_bytree=colsample_bytree,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Fit the final model\n",
        "final_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions using the final model\n",
        "y_pred_final = final_model.predict(X_test)\n",
        "\n",
        "# Evaluate the final model\n",
        "final_mae = mean_absolute_error(y_test, y_pred_final)\n",
        "final_mse = mean_squared_error(y_test, y_pred_final)\n",
        "final_r2 = r2_score(y_test, y_pred_final)\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(f\"Final Model - Mean Absolute Error (MAE): {final_mae}\")\n",
        "print(f\"Final Model - Mean Squared Error (MSE): {final_mse}\")\n",
        "print(f\"Final Model - R2 Score: {final_r2}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HpltR2Uk9kFb",
        "outputId": "c4b274a3-1a37-4d16-cfac-3acd0ab76692"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "import pandas as pd\n",
        "\n",
        "# Assume the following variables are already set up:\n",
        "# df_final, target_column, X, y, X_train, X_test, y_train, y_test\n",
        "\n",
        "# Define the objective function for ACO\n",
        "def objective_function(params):\n",
        "    learning_rate, max_depth, n_estimators, subsample, colsample_bytree = params\n",
        "\n",
        "    # Train XGBoost model with the given parameters\n",
        "    model = xgb.XGBRegressor(\n",
        "        learning_rate=learning_rate,\n",
        "        max_depth=int(max_depth),  # max_depth should be an integer\n",
        "        n_estimators=int(n_estimators),  # n_estimators should be an integer\n",
        "        subsample=subsample,\n",
        "        colsample_bytree=colsample_bytree,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on the test set\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate the mean absolute error (MAE)\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "    return mae  # We want to minimize MAE\n",
        "\n",
        "# Simple ACO Hyperparameter Optimization\n",
        "class AntColonyOptimizer:\n",
        "    def __init__(self, objective_function, bounds, n_ants=5, n_iter=10, alpha=1, beta=1, rho=0.9):\n",
        "        self.objective_function = objective_function\n",
        "        self.bounds = bounds\n",
        "        self.n_ants = n_ants\n",
        "        self.n_iter = n_iter\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.rho = rho\n",
        "\n",
        "        # Initialize pheromone levels (same for all dimensions)\n",
        "        self.pheromone = np.ones((len(bounds), 10))  # 10 is an arbitrary precision size\n",
        "        self.best_score = float('inf')\n",
        "        self.best_solution = None\n",
        "\n",
        "    def optimize(self):\n",
        "        for iteration in range(self.n_iter):\n",
        "            solutions = []\n",
        "            scores = []\n",
        "            for _ in range(self.n_ants):\n",
        "                solution = self.construct_solution()\n",
        "                score = self.objective_function(solution)\n",
        "                solutions.append(solution)\n",
        "                scores.append(score)\n",
        "                if score < self.best_score:\n",
        "                    self.best_score = score\n",
        "                    self.best_solution = solution\n",
        "            self.update_pheromone(solutions, scores)\n",
        "        return self.best_solution, self.best_score\n",
        "\n",
        "    def construct_solution(self):\n",
        "        solution = []\n",
        "        for i, (lb, ub) in enumerate(self.bounds):\n",
        "            prob = self.pheromone[i] ** self.alpha\n",
        "            prob /= np.sum(prob)  # Normalize the probabilities\n",
        "\n",
        "            # Pick the index of the best probability based on pheromone level\n",
        "            index = np.random.choice(range(10), p=prob)\n",
        "            value = lb + (index / 9.0) * (ub - lb)  # Scale it to the actual bounds\n",
        "            solution.append(value)\n",
        "        return solution\n",
        "\n",
        "    def update_pheromone(self, solutions, scores):\n",
        "        # Update pheromones based on the best solutions\n",
        "        delta_pheromone = np.zeros_like(self.pheromone)\n",
        "        for solution, score in zip(solutions, scores):\n",
        "            for i, value in enumerate(solution):\n",
        "                delta_pheromone[i] += (self.best_score - score) / self.best_score  # Reward better solutions\n",
        "\n",
        "        # Apply pheromone evaporation\n",
        "        self.pheromone *= self.rho\n",
        "        # Add new pheromone\n",
        "        self.pheromone += delta_pheromone\n",
        "\n",
        "# Define the bounds for the hyperparameters\n",
        "# Bounds: [learning_rate, max_depth, n_estimators, subsample, colsample_bytree]\n",
        "lb = [0.01, 3, 50, 0.5, 0.5]  # Lower bounds of the parameters\n",
        "ub = [0.1, 8, 100, 0.9, 0.9]  # Upper bounds of the parameters (narrowed range)\n",
        "\n",
        "# Perform ACO optimization with minimal swarm size and iterations\n",
        "aco_optimizer = AntColonyOptimizer(objective_function, bounds=list(zip(lb, ub)), n_ants=5, n_iter=10)\n",
        "best_params, best_score = aco_optimizer.optimize()\n",
        "\n",
        "# Print the best parameters found by ACO\n",
        "print(f\"Best parameters found by ACO: {best_params}\")\n",
        "\n",
        "# Extract the best parameters\n",
        "learning_rate, max_depth, n_estimators, subsample, colsample_bytree = best_params\n",
        "\n",
        "# Train the final XGBoost model using the optimized parameters\n",
        "final_model = xgb.XGBRegressor(\n",
        "    learning_rate=learning_rate,\n",
        "    max_depth=int(max_depth),\n",
        "    n_estimators=int(n_estimators),\n",
        "    subsample=subsample,\n",
        "    colsample_bytree=colsample_bytree,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Fit the final model\n",
        "final_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions using the final model\n",
        "y_pred_final = final_model.predict(X_test)\n",
        "\n",
        "# Evaluate the final model\n",
        "final_mae = mean_absolute_error(y_test, y_pred_final)\n",
        "final_mse = mean_squared_error(y_test, y_pred_final)\n",
        "final_r2 = r2_score(y_test, y_pred_final)\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(f\"Final Model - Mean Absolute Error (MAE): {final_mae}\")\n",
        "print(f\"Final Model - Mean Squared Error (MSE): {final_mse}\")\n",
        "print(f\"Final Model - R2 Score: {final_r2}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2d_rDQU5-SGI",
        "outputId": "337e838b-04c6-4294-e8ff-d465e702db41"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "import pandas as pd\n",
        "\n",
        "# Assume the following variables are already set up:\n",
        "# df_final, target_column, X, y, X_train, X_test, y_train, y_test\n",
        "\n",
        "# Define the objective function for Hill Climbing\n",
        "def objective_function(params):\n",
        "    learning_rate, max_depth, n_estimators, subsample, colsample_bytree = params\n",
        "\n",
        "    # Train XGBoost model with the given parameters\n",
        "    model = xgb.XGBRegressor(\n",
        "        learning_rate=learning_rate,\n",
        "        max_depth=int(max_depth),  # max_depth should be an integer\n",
        "        n_estimators=int(n_estimators),  # n_estimators should be an integer\n",
        "        subsample=subsample,\n",
        "        colsample_bytree=colsample_bytree,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on the test set\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate the mean absolute error (MAE)\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "    return mae  # We want to minimize MAE\n",
        "\n",
        "# Hill Climbing Hyperparameter Optimization\n",
        "def hill_climbing(objective_function, bounds, max_iterations=5, step_size=0.01):\n",
        "    # Generate an initial random solution within bounds\n",
        "    current_solution = [np.random.uniform(lb, ub) for lb, ub in bounds]\n",
        "    current_score = objective_function(current_solution)\n",
        "\n",
        "    best_solution = current_solution\n",
        "    best_score = current_score\n",
        "\n",
        "    for _ in range(max_iterations):\n",
        "        # Generate neighbors by slightly modifying the current solution\n",
        "        neighbors = []\n",
        "        for i, (lb, ub) in enumerate(bounds):\n",
        "            neighbor = current_solution.copy()\n",
        "            change = np.random.uniform(-step_size, step_size)\n",
        "            neighbor[i] = np.clip(neighbor[i] + change, lb, ub)  # Ensure values are within bounds\n",
        "            neighbors.append(neighbor)\n",
        "\n",
        "        # Evaluate the neighbors and select the best one\n",
        "        for neighbor in neighbors:\n",
        "            score = objective_function(neighbor)\n",
        "            if score < best_score:\n",
        "                best_score = score\n",
        "                best_solution = neighbor\n",
        "\n",
        "        # Update the current solution\n",
        "        current_solution = best_solution\n",
        "\n",
        "    return best_solution, best_score\n",
        "\n",
        "# Define the bounds for the hyperparameters\n",
        "# Bounds: [learning_rate, max_depth, n_estimators, subsample, colsample_bytree]\n",
        "lb = [0.01, 3, 50, 0.5, 0.5]  # Lower bounds of the parameters\n",
        "ub = [0.1, 8, 100, 0.9, 0.9]  # Upper bounds of the parameters (narrowed range)\n",
        "\n",
        "# Perform Hill Climbing optimization with minimal iterations (5 iterations)\n",
        "bounds = list(zip(lb, ub))\n",
        "best_params, best_score = hill_climbing(objective_function, bounds, max_iterations=5)\n",
        "\n",
        "# Print the best parameters found by Hill Climbing\n",
        "print(f\"Best parameters found by Hill Climbing: {best_params}\")\n",
        "\n",
        "# Extract the best parameters\n",
        "learning_rate, max_depth, n_estimators, subsample, colsample_bytree = best_params\n",
        "\n",
        "# Train the final XGBoost model using the optimized parameters\n",
        "final_model = xgb.XGBRegressor(\n",
        "    learning_rate=learning_rate,\n",
        "    max_depth=int(max_depth),\n",
        "    n_estimators=int(n_estimators),\n",
        "    subsample=subsample,\n",
        "    colsample_bytree=colsample_bytree,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Fit the final model\n",
        "final_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions using the final model\n",
        "y_pred_final = final_model.predict(X_test)\n",
        "\n",
        "# Evaluate the final model\n",
        "final_mae = mean_absolute_error(y_test, y_pred_final)\n",
        "final_mse = mean_squared_error(y_test, y_pred_final)\n",
        "final_r2 = r2_score(y_test, y_pred_final)\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(f\"Final Model - Mean Absolute Error (MAE): {final_mae}\")\n",
        "print(f\"Final Model - Mean Squared Error (MSE): {final_mse}\")\n",
        "print(f\"Final Model - R2 Score: {final_r2}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yuMuH-hn1TSo",
        "outputId": "35f148e5-f107-40ed-e8a9-66a254db9700"
      },
      "outputs": [],
      "source": [
        "pip install pyswarm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBu7A6dWiQrP",
        "outputId": "f79b4089-3689-445f-dd89-b199ac7c955f"
      },
      "outputs": [],
      "source": [
        "# Step 6: Initialize and train the CatBoostRegressor model\n",
        "from catboost import CatBoostRegressor\n",
        "\n",
        "model = CatBoostRegressor(iterations=1000, depth=10, learning_rate=0.1, loss_function='RMSE',  random_state=42, verbose=100)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 7: Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 8: Evaluate the model\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f'Mean Absolute Error (MAE): {mae}')\n",
        "print(f'Mean Squared Error (MSE): {mse}')\n",
        "print(f'R2 Score: {r2}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taxdN5rzifZq",
        "outputId": "52e168d4-3358-459c-bcf2-911c567c77b6"
      },
      "outputs": [],
      "source": [
        "# Step 6: Initialize and train the LGBMRegressor model\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Initialize the LGBMRegressor model\n",
        "model = LGBMRegressor(n_estimators=1000,\n",
        "                      max_depth=10,\n",
        "                      learning_rate=0.1,\n",
        "                      objective='regression',\n",
        "                      random_state=42,\n",
        "                      verbose=100)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 7: Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 8: Evaluate the model\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f'Mean Absolute Error (MAE): {mae}')\n",
        "print(f'Mean Squared Error (MSE): {mse}')\n",
        "print(f'R2 Score: {r2}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFOFUo6kpJc-",
        "outputId": "2c480429-c65f-49ff-bcfa-38c383416b76"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "df_final = df_no_outliers.copy()\n",
        "\n",
        "# Step 1: Convert all categorical columns to string type\n",
        "categorical_cols = df_final.select_dtypes(include=['object','bool']).columns\n",
        "df_final[categorical_cols] = df_final[categorical_cols].astype(str)\n",
        "\n",
        "# Step 2: Define the target and features\n",
        "target_column = '119. PRODUCTION WSTE (8.1-8.7)'  # Assuming this is the target column\n",
        "X = df_final.drop(columns=[target_column])  # Features (excluding target)\n",
        "y = df_final[target_column]  # Target (Production Waste)\n",
        "\n",
        "# Step 3: Apply Label Encoding to all categorical columns\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "for col in X.select_dtypes(include=['object','category']).columns:\n",
        "    X[col] = label_encoder.fit_transform(X[col])\n",
        "\n",
        "# Step 4: Apply Standard Scaling to the features\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Only scale the numeric columns (exclude the target and categorical columns)\n",
        "numeric_cols = X.select_dtypes(include=['float64', 'int64']).columns\n",
        "X[numeric_cols] = scaler.fit_transform(X[numeric_cols])\n",
        "\n",
        "# Step 5: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define base models\n",
        "xgb_model = XGBRegressor(n_estimators=100)\n",
        "gbr_model = GradientBoostingRegressor(n_estimators=100,)\n",
        "rf_model = RandomForestRegressor(n_estimators=100)\n",
        "\n",
        "# Step 6: Train the base models\n",
        "xgb_model.fit(X_train, y_train)\n",
        "gbr_model.fit(X_train, y_train)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Step 7: Make predictions from the base models on the training and test set\n",
        "xgb_train_pred = xgb_model.predict(X_train)\n",
        "gbr_train_pred = gbr_model.predict(X_train)\n",
        "rf_train_pred = rf_model.predict(X_train)\n",
        "\n",
        "xgb_test_pred = xgb_model.predict(X_test)\n",
        "gbr_test_pred = gbr_model.predict(X_test)\n",
        "rf_test_pred = rf_model.predict(X_test)\n",
        "\n",
        "# Step 8: Stack the predictions (use the predictions as features for the meta-model)\n",
        "stacked_train_predictions = np.column_stack((xgb_train_pred ,rf_train_pred))\n",
        "stacked_test_predictions = np.column_stack((xgb_test_pred, rf_test_pred))\n",
        "\n",
        "# Step 9: Define the meta-model (GradientBoostingRegressor)\n",
        "meta_model = GradientBoostingRegressor(n_estimators=100)\n",
        "\n",
        "# Step 10: Train the meta-model using the stacked predictions\n",
        "meta_model.fit(stacked_train_predictions, y_train)\n",
        "\n",
        "# Step 11: Make predictions using the meta-model on the test set\n",
        "final_predictions = meta_model.predict(stacked_test_predictions)\n",
        "\n",
        "# Step 12: Calculate RMSE and MAE for the training and test set\n",
        "train_rmse = np.sqrt(mean_squared_error(y_train, meta_model.predict(stacked_train_predictions)))\n",
        "train_mae = mean_absolute_error(y_train, meta_model.predict(stacked_train_predictions))\n",
        "test_rmse = np.sqrt(mean_squared_error(y_test, final_predictions))\n",
        "test_mae = mean_absolute_error(y_test, final_predictions)\n",
        "\n",
        "print(f'Training RMSE: {train_rmse}')\n",
        "print(f'Training MAE: {train_mae}')\n",
        "print(f'Test RMSE: {test_rmse}')\n",
        "print(f'Test MAE: {test_mae}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMl8uzeommXA",
        "outputId": "c4f86c9c-8488-45d9-b6a5-b4a76e25fbc0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
        "\n",
        "# Define models with weights (XGBoost, GradientBoosting, RandomForest)\n",
        "models_with_weights = {\n",
        "    'XGBRegressor': (XGBRegressor(n_estimators=100), 0.1),\n",
        "    'GradientBoostingRegressor': (GradientBoostingRegressor(n_estimators=100), 0.8),\n",
        "    'RandomForestRegressor': (RandomForestRegressor(n_estimators=100), 0.1)\n",
        "}\n",
        "\n",
        "# Initialize DataFrame for storing predictions\n",
        "blended_train_pred = pd.DataFrame()\n",
        "blended_test_pred = pd.DataFrame()\n",
        "\n",
        "# Loop over models and make predictions\n",
        "for model_name, (model, weight) in models_with_weights.items():\n",
        "    # Fit model and generate predictions\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Flatten predictions and apply weight\n",
        "    blended_train_pred[model_name] = weight * model.predict(X_train).squeeze()\n",
        "    blended_test_pred[model_name] = weight * model.predict(X_test).squeeze()\n",
        "\n",
        "print(f'blended_train_pred.shape: {blended_train_pred.shape}')\n",
        "print(f'blended_test_pred.shape: {blended_test_pred.shape}')\n",
        "\n",
        "# Calculate RMSE for the training data\n",
        "def rmse(y_true, y_pred):\n",
        "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "train_rmse = rmse(y_train, blended_train_pred.sum(axis='columns'))\n",
        "print(f'Training RMSE: {train_rmse}')\n",
        "\n",
        "# Calculate MAE for the training data\n",
        "train_mae = mean_absolute_error(y_train, blended_train_pred.sum(axis='columns'))\n",
        "print(f'Training MAE: {train_mae}')\n",
        "\n",
        "# Generate final test predictions\n",
        "test_pred = np.floor(np.expm1(blended_test_pred.sum(axis='columns')))\n",
        "\n",
        "# Calculate RMSE for test predictions\n",
        "test_rmse = rmse(y_test, blended_test_pred.sum(axis='columns'))\n",
        "print(f'Test RMSE: {test_rmse}')\n",
        "\n",
        "# Calculate MAE for test predictions\n",
        "test_mae = mean_absolute_error(y_test, blended_test_pred.sum(axis='columns'))\n",
        "print(f'Test MAE: {test_mae}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fp8cl-n00mFq",
        "outputId": "d5a8c63f-8e67-4a92-eded-cbc0f3265c10"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
        "\n",
        "# Define models with weights (XGBoost, GradientBoosting, RandomForest)\n",
        "models_with_weights = {\n",
        "    'XGBRegressor': (XGBRegressor(n_estimators=100), 0.1),\n",
        "    'GradientBoostingRegressor': (GradientBoostingRegressor(n_estimators=100), 0.8),\n",
        "    'RandomForestRegressor': (RandomForestRegressor(n_estimators=100), 0.1)\n",
        "}\n",
        "\n",
        "# Initialize DataFrame for storing predictions\n",
        "blended_train_pred = pd.DataFrame()\n",
        "blended_test_pred = pd.DataFrame()\n",
        "\n",
        "# Loop over models and make predictions\n",
        "for model_name, (model, weight) in models_with_weights.items():\n",
        "    # Fit model and generate predictions\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Flatten predictions and apply weight\n",
        "    blended_train_pred[model_name] = weight * model.predict(X_train).squeeze()\n",
        "    blended_test_pred[model_name] = weight * model.predict(X_test).squeeze()\n",
        "\n",
        "print(f'blended_train_pred.shape: {blended_train_pred.shape}')\n",
        "print(f'blended_test_pred.shape: {blended_test_pred.shape}')\n",
        "\n",
        "# Calculate RMSE for the training data\n",
        "def rmse(y_true, y_pred):\n",
        "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "train_rmse = rmse(y_train, blended_train_pred.sum(axis='columns'))\n",
        "print(f'Training RMSE: {train_rmse}')\n",
        "\n",
        "# Calculate MAE for the training data\n",
        "train_mae = mean_absolute_error(y_train, blended_train_pred.sum(axis='columns'))\n",
        "print(f'Training MAE: {train_mae}')\n",
        "\n",
        "# Generate final test predictions\n",
        "test_pred = np.floor(np.expm1(blended_test_pred.sum(axis='columns')))\n",
        "\n",
        "# Calculate RMSE for test predictions\n",
        "test_rmse = rmse(y_test, blended_test_pred.sum(axis='columns'))\n",
        "print(f'Test RMSE: {test_rmse}')\n",
        "\n",
        "# Calculate MAE for test predictions\n",
        "test_mae = mean_absolute_error(y_test, blended_test_pred.sum(axis='columns'))\n",
        "print(f'Test MAE: {test_mae}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xv2zeQSNX7n",
        "outputId": "284d92a2-e81b-424d-ab05-c07e6233c96b"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Assuming df_no_outliers is already preprocessed\n",
        "df_final = df_no_outliers.copy()\n",
        "\n",
        "# Step 1: Convert all categorical columns to string type\n",
        "categorical_cols = df_final.select_dtypes(include=['object','bool']).columns\n",
        "df_final[categorical_cols] = df_final[categorical_cols].astype(str)\n",
        "\n",
        "# Step 2: Define the target and features\n",
        "target_column = '119. PRODUCTION WSTE (8.1-8.7)'  # Assuming this is the target column\n",
        "X = df_final.drop(columns=[target_column])  # Features (excluding target)\n",
        "y = df_final[target_column]  # Target (Production Waste)\n",
        "\n",
        "# Step 3: Apply Label Encoding to all categorical columns\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "for col in X.select_dtypes(include=['object','category']).columns:\n",
        "    X[col] = label_encoder.fit_transform(X[col])\n",
        "\n",
        "# Step 4: Apply Standard Scaling to the features\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Only scale the numeric columns (exclude the target and categorical columns)\n",
        "numeric_cols = X.select_dtypes(include=['float64', 'int64']).columns\n",
        "X[numeric_cols] = scaler.fit_transform(X[numeric_cols])\n",
        "\n",
        "# Step 5: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 6: Initialize and train the RandomForestRegressor model\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "# Step 7: Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 8: Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 9: Evaluate the model\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f'Mean Absolute Error (MAE): {mae}')\n",
        "print(f'Mean Squared Error (MSE): {mse}')\n",
        "print(f'R2 Score: {r2}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdDmqpnpNgqc",
        "outputId": "983774e9-398a-439f-d61f-b902a7689023"
      },
      "outputs": [],
      "source": [
        "df_final.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VHZGEUVNv96",
        "outputId": "98455fb7-648e-4e5e-de91-b5f00024e0eb"
      },
      "outputs": [],
      "source": [
        "df_final['119. PRODUCTION WSTE (8.1-8.7)'].mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dio1EgNWPI1p",
        "outputId": "23a5ece0-8706-419e-a279-5f9c3eff42fa"
      },
      "outputs": [],
      "source": [
        "np.sqrt(mse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4aMres1PMb-",
        "outputId": "38d06e0a-62a3-4078-a3fc-e3b038cf421c"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "# Step 6: Initialize and train the GradientBoostingRegressor model\n",
        "model = GradientBoostingRegressor(n_estimators=100, random_state=4)\n",
        "# X_train=X_train.drop(['Total Waste to Facility Ratio','Total Emissions to Facility Ratio'],axis=1)\n",
        "# X_test=X_test.drop(['Total Waste to Facility Ratio','Total Emissions to Facility Ratio'],axis=1)\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f'Mean Absolute Error (MAE): {mae}')\n",
        "print(f'Mean Squared Error (MSE): {mse}')\n",
        "print(f'R2 Score: {r2}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqQzRFhuPYNi",
        "outputId": "cc980849-0c2d-4131-9c15-43aee9f96a00"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "# Step 6: Initialize and train the XGBoost model\n",
        "model = xgb.XGBRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f'Mean Absolute Error (MAE): {mae}')\n",
        "print(f'Mean Squared Error (MSE): {mse}')\n",
        "print(f'R2 Score: {r2}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EV6XeZyGPeLt",
        "outputId": "ce50eb02-5e92-443f-ef25-3f16c7c7e965"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Step 6: Initialize and train the Linear Regression model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f'Mean Absolute Error (MAE): {mae}')\n",
        "print(f'Mean Squared Error (MSE): {mse}')\n",
        "print(f'R2 Score: {r2}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOchR59kQcnu"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVR\n",
        "\n",
        "# Step 6: Initialize and train the Support Vector Regressor model\n",
        "model = SVR()\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f'Mean Absolute Error (MAE): {mae}')\n",
        "print(f'Mean Squared Error (MSE): {mse}')\n",
        "print(f'R2 Score: {r2}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdUAeJn_QSJL",
        "outputId": "2475d60a-f7b9-494f-da72-5b2e430bcaa2"
      },
      "outputs": [],
      "source": [
        "# Step 6: Initialize and train the CatBoostRegressor model\n",
        "from catboost import CatBoostRegressor\n",
        "\n",
        "model = CatBoostRegressor(iterations=1000, depth=10, learning_rate=0.1, loss_function='RMSE',  random_state=42, verbose=100)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 7: Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 8: Evaluate the model\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f'Mean Absolute Error (MAE): {mae}')\n",
        "print(f'Mean Squared Error (MSE): {mse}')\n",
        "print(f'R2 Score: {r2}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "fSpIFnf1QT-V",
        "outputId": "f9e22bbd-17fb-4c3e-fd20-e9bc73fea23d"
      },
      "outputs": [],
      "source": [
        "from pyswarm import pso\n",
        "import xgboost as xgb\n",
        "\n",
        "\n",
        "def objective_function(params):\n",
        "    \"\"\"\n",
        "    Fungsi objektif yang digunakan oleh PSO untuk mengevaluasi model XGBoost.\n",
        "    params[0]: learning_rate\n",
        "    params[1]: max_depth\n",
        "    params[2]: n_estimators\n",
        "    params[3]: subsample\n",
        "    \"\"\"\n",
        "    learning_rate = params[0]\n",
        "    max_depth = int(params[1])\n",
        "    n_estimators = int(params[2])\n",
        "    subsample = params[3]\n",
        "\n",
        "    # Inisialisasi model XGBoost\n",
        "    model = xgb.XGBRegressor(\n",
        "        learning_rate=learning_rate,\n",
        "        max_depth=max_depth,\n",
        "        n_estimators=n_estimators,\n",
        "        subsample=subsample,\n",
        "        objective='reg:squarederror',\n",
        "        random_state=42,\n",
        "        verbosity=0\n",
        "    )\n",
        "\n",
        "    # Latih model\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Prediksi pada data test\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Hitung metrik MAE (Mean Absolute Error)\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    return mae  # Kita optimalkan MAE yang lebih rendah\n",
        "\n",
        "# Langkah 6: Tentukan batas untuk parameter PSO\n",
        "lb = [0.01, 3, 50, 0.6]  # Batas bawah [learning_rate, max_depth, n_estimators, subsample]\n",
        "ub = [0.2, 15, 500, 1.0]  # Batas atas [learning_rate, max_depth, n_estimators, subsample]\n",
        "\n",
        "# Langkah 7: Terapkan PSO untuk menemukan parameter terbaik\n",
        "best_params, best_obj_value = pso(objective_function, lb, ub, swarmsize=10, maxiter=5)\n",
        "\n",
        "# Menampilkan parameter terbaik yang ditemukan oleh PSO\n",
        "print(f\"Best Parameters: Learning Rate={best_params[0]}, Max Depth={int(best_params[1])}, N Estimators={int(best_params[2])}, Subsample={best_params[3]}\")\n",
        "print(f\"Best Objective Value (MAE): {best_obj_value}\")\n",
        "\n",
        "# Langkah 8: Gunakan parameter terbaik untuk melatih model akhir\n",
        "best_learning_rate = best_params[0]\n",
        "best_max_depth = int(best_params[1])\n",
        "best_n_estimators = int(best_params[2])\n",
        "best_subsample = best_params[3]\n",
        "\n",
        "final_model = xgb.XGBRegressor(\n",
        "    learning_rate=best_learning_rate,\n",
        "    max_depth=best_max_depth,\n",
        "    n_estimators=best_n_estimators,\n",
        "    subsample=best_subsample,\n",
        "    objective='reg:squarederror',\n",
        "    random_state=42,\n",
        "    verbosity=0\n",
        ")\n",
        "\n",
        "# Latih model dengan parameter terbaik\n",
        "final_model.fit(X_train, y_train)\n",
        "\n",
        "# Prediksi dengan model terbaik\n",
        "y_pred_final = final_model.predict(X_test)\n",
        "\n",
        "# Evaluasi model akhir\n",
        "mae = mean_absolute_error(y_test, y_pred_final)\n",
        "mse = mean_squared_error(y_test, y_pred_final)\n",
        "r2 = r2_score(y_test, y_pred_final)\n",
        "\n",
        "# Metrik evaluasi\n",
        "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
        "print(f\"Mean Squared Error (MSE): {mse}\")\n",
        "print(f\"R2 Score: {r2}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pD1h1zhxTQX8",
        "outputId": "33e22e84-5cb3-44c1-e4d1-8e29200de6ec"
      },
      "outputs": [],
      "source": [
        "def objective_function(params):\n",
        "    \"\"\"\n",
        "    Fungsi objektif yang digunakan untuk mengevaluasi model XGBoost dengan parameter tertentu.\n",
        "    params[0]: learning_rate\n",
        "    params[1]: max_depth\n",
        "    params[2]: n_estimators\n",
        "    params[3]: subsample\n",
        "    \"\"\"\n",
        "    learning_rate = params[0]\n",
        "    max_depth = int(params[1])\n",
        "    n_estimators = int(params[2])\n",
        "    subsample = params[3]\n",
        "\n",
        "    # Inisialisasi model XGBoost\n",
        "    model = xgb.XGBRegressor(\n",
        "        learning_rate=learning_rate,\n",
        "        max_depth=max_depth,\n",
        "        n_estimators=n_estimators,\n",
        "        subsample=subsample,\n",
        "        objective='reg:squarederror',\n",
        "        random_state=42,\n",
        "        verbosity=0\n",
        "    )\n",
        "\n",
        "    # Latih model\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Prediksi pada data test\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Hitung metrik MAE (Mean Absolute Error)\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    return mae  # Kita optimalkan MAE yang lebih rendah\n",
        "\n",
        "# Langkah 6: Inisialisasi parameter acuan untuk Hill Climbing\n",
        "current_params = [0.1, 6, 100, 0.8]  # Inisialisasi dengan beberapa nilai parameter awal\n",
        "current_score = objective_function(current_params)\n",
        "\n",
        "# Langkah 7: Hill Climbing Loop\n",
        "def hill_climb():\n",
        "    global current_params, current_score\n",
        "    step_size = 0.01  # Langkah perubahan untuk setiap parameter\n",
        "    max_iterations = 100  # Maksimal iterasi pencarian\n",
        "\n",
        "    for iteration in range(max_iterations):\n",
        "        # Generate new parameters by making small changes to the current ones\n",
        "        new_params = current_params.copy()\n",
        "\n",
        "        # Cobalah perubahan kecil untuk setiap parameter\n",
        "        new_params[0] += np.random.uniform(-step_size, step_size)  # learning_rate\n",
        "        new_params[1] = int(new_params[1] + np.random.choice([-1, 1]) * np.random.randint(1, 3))  # max_depth\n",
        "        new_params[2] = int(new_params[2] + np.random.choice([-1, 1]) * np.random.randint(1, 20)))  # n_estimators\n",
        "        new_params[3] += np.random.uniform(-step_size, step_size)  # subsample\n",
        "\n",
        "        # Bound parameters within their limits\n",
        "        new_params[0] = np.clip(new_params[0], 0.01, 0.2)\n",
        "        new_params[1] = np.clip(new_params[1], 3, 15)\n",
        "        new_params[2] = np.clip(new_params[2], 50, 500)\n",
        "        new_params[3] = np.clip(new_params[3], 0.6, 1.0)\n",
        "\n",
        "        # Evaluasi parameter baru\n",
        "        new_score = objective_function(new_params)\n",
        "\n",
        "        # Jika parameter baru lebih baik, lakukan pembaruan\n",
        "        if new_score < current_score:\n",
        "            print(f\"Iteration {iteration + 1}: Improved MAE = {new_score} with parameters {new_params}\")\n",
        "            current_params = new_params\n",
        "            current_score = new_score\n",
        "\n",
        "# Langkah 8: Jalankan Hill Climbing\n",
        "hill_climb()\n",
        "\n",
        "# Langkah 9: Gunakan parameter terbaik yang ditemukan untuk melatih model akhir\n",
        "best_learning_rate = current_params[0]\n",
        "best_max_depth = int(current_params[1])\n",
        "best_n_estimators = int(current_params[2])\n",
        "best_subsample = current_params[3]\n",
        "\n",
        "final_model = xgb.XGBRegressor(\n",
        "    learning_rate=best_learning_rate,\n",
        "    max_depth=best_max_depth,\n",
        "    n_estimators=best_n_estimators,\n",
        "    subsample=best_subsample,\n",
        "    objective='reg:squarederror',\n",
        "    random_state=42,\n",
        "    verbosity=0\n",
        ")\n",
        "\n",
        "# Latih model dengan parameter terbaik\n",
        "final_model.fit(X_train, y_train)\n",
        "\n",
        "# Prediksi dengan model terbaik\n",
        "y_pred_final = final_model.predict(X_test)\n",
        "\n",
        "# Evaluasi model akhir\n",
        "mae = mean_absolute_error(y_test, y_pred_final)\n",
        "mse = mean_squared_error(y_test, y_pred_final)\n",
        "r2 = r2_score(y_test, y_pred_final)\n",
        "\n",
        "# Metrik evaluasi\n",
        "print(f\"Final Model Mean Absolute Error (MAE): {mae}\")\n",
        "print(f\"Final Model Mean Squared Error (MSE): {mse}\")\n",
        "print(f\"Final Model R2 Score: {r2}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IzrHznKOTZbE",
        "outputId": "0a75bc9f-1e05-4103-b5df-d9b3414af0cb"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Extract feature importances\n",
        "feature_importances = model.feature_importances_\n",
        "features = X_train.columns  # Assuming X_train is a DataFrame\n",
        "\n",
        "# Create a DataFrame for better visualization\n",
        "importance_df = pd.DataFrame({\n",
        "    'Feature': features,\n",
        "    'Importance': feature_importances\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Plot the top 10 feature importances\n",
        "plt.figure(figsize=(10, 6))\n",
        "top_features = importance_df.head(10)\n",
        "plt.barh(top_features['Feature'], top_features['Importance'], color='#5038bc')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.title('Top 10 Feature Importances', fontsize=16)\n",
        "plt.xlabel('Importance', fontsize=14)\n",
        "plt.ylabel('Feature', fontsize=14)\n",
        "plt.savefig(\"Top 10 Feature_GB\")\n",
        "plt.show()\n",
        "\n",
        "# Scatter plot for actual vs predicted values\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(y_test, y_pred, alpha=0.6, color='#5038bc')\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
        "plt.title('Actual vs Predicted Values', fontsize=16)\n",
        "plt.xlabel('Actual Values', fontsize=14)\n",
        "plt.ylabel('Predicted Values', fontsize=14)\n",
        "plt.grid(True)\n",
        "plt.savefig(\"pred\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "id": "lizSC-BW_AeZ",
        "outputId": "18323883-2713-477f-82cf-19dea3b2177a"
      },
      "outputs": [],
      "source": [
        "import shap\n",
        "\n",
        "# Initialize SHAP explainer\n",
        "explainer = shap.Explainer(model, X_train)\n",
        "shap_values = explainer(X_test)\n",
        "\n",
        "# Limit to top 5 features based on mean absolute SHAP values\n",
        "shap_values_df = pd.DataFrame({\n",
        "    \"Feature\": X_train.columns,\n",
        "    \"Mean Abs SHAP\": np.abs(shap_values.values).mean(axis=0)\n",
        "}).sort_values(by=\"Mean Abs SHAP\", ascending=False)\n",
        "\n",
        "top_5_features = shap_values_df.head(5)[\"Feature\"].tolist()\n",
        "\n",
        "# Filter SHAP values and X_test for the top 5 features\n",
        "top_5_indices = [X_train.columns.get_loc(feature) for feature in top_5_features]\n",
        "filtered_shap_values = shap_values[:, top_5_indices]\n",
        "filtered_X_test = X_test[top_5_features]\n",
        "\n",
        "# SHAP Summary Plot (Bar)\n",
        "shap.summary_plot(filtered_shap_values, filtered_X_test, plot_type=\"bar\", color=\"#5038bc\")\n",
        "\n",
        "# SHAP Beeswarm Plot (Detailed Insights)\n",
        "shap.summary_plot(filtered_shap_values, filtered_X_test, color=\"#5038bc\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        },
        "id": "ljtx5I-7Br0U",
        "outputId": "a93d2e79-a6dd-4302-a5ff-017426f24bdc"
      },
      "outputs": [],
      "source": [
        "import shap\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Initialize SHAP explainer\n",
        "explainer = shap.Explainer(model, X_train)\n",
        "shap_values = explainer(X_test)\n",
        "\n",
        "# Limit to top 5 features based on mean absolute SHAP values\n",
        "shap_values_df = pd.DataFrame({\n",
        "    \"Feature\": X_train.columns,\n",
        "    \"Mean Abs SHAP\": np.abs(shap_values.values).mean(axis=0)\n",
        "}).sort_values(by=\"Mean Abs SHAP\", ascending=False)\n",
        "\n",
        "top_5_features = shap_values_df.head(10)[\"Feature\"].tolist()\n",
        "\n",
        "# Filter SHAP values and X_test for the top 5 features\n",
        "top_5_indices = [X_train.columns.get_loc(feature) for feature in top_5_features]\n",
        "filtered_shap_values = shap_values[:, top_5_indices]\n",
        "filtered_X_test = X_test[top_5_features]\n",
        "\n",
        "# SHAP Beeswarm Plot (Detailed Insights for Top 5 Features)\n",
        "plt.figure()  # Create a new figure for saving\n",
        "shap.summary_plot(filtered_shap_values, filtered_X_test, color=\"#5038bc\", show=False)  # Prevent auto-show\n",
        "plt.savefig(\"shap_beeswarm_plot.png\", bbox_inches=\"tight\", dpi=300)  # Save the plot\n",
        "plt.show()  # Display the plot\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634
        },
        "id": "y39BO7m8CERA",
        "outputId": "b5578a39-bc62-4562-bb0d-19f5c4c850b6"
      },
      "outputs": [],
      "source": [
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure()  # Create a new figure for saving\n",
        "# SHAP Waterfall Plot for a specific instance (e.g., the first test sample)\n",
        "shap.waterfall_plot(\n",
        "    shap.Explanation(\n",
        "        values=filtered_shap_values.values[0],\n",
        "        base_values=explainer.expected_value,\n",
        "        feature_names=top_5_features\n",
        "    )\n",
        ")\n",
        "plt.savefig(\"shap_waterfall_plot.png\", bbox_inches=\"tight\", dpi=300)  # Save the plot\n",
        "plt.show()  # Display the plot\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3mZJoB6CNgJ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
